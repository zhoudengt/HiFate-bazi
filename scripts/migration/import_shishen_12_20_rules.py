#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¼å…¥ åç¥å‘½æ ¼12.20.xlsx è§„åˆ™åˆ°æ•°æ®åº“

è§„åˆ™ç±»å‹: shishen (åç¥å‘½æ ¼)
"""

from __future__ import annotations

import argparse
import json
import os
import sys
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, PROJECT_ROOT)

# å°è¯•å¯¼å…¥ pandas
try:
    import pandas as pd
except ImportError:
    print("âŒ éœ€è¦å®‰è£… pandas: pip install pandas openpyxl")
    sys.exit(1)

# å¯¼å…¥è§£æå™¨ï¼ˆä½¿ç”¨æœ€æ–°çš„RuleParserï¼‰
from scripts.migration.import_2025_12_03_rules import RuleParser, RuleRecord, ParseResult

XLSX_FILE = os.path.join(PROJECT_ROOT, "docs", "upload", "åç¥å‘½æ ¼12.20.xlsx")
OUTPUT_JSON = os.path.join(PROJECT_ROOT, "docs", "æœªè§£æè§„åˆ™_åç¥å‘½æ ¼12.20_è¯¦ç»†è¯´æ˜.json")

# è§„åˆ™ç±»å‹æ˜ å°„ï¼ˆåç¥å‘½æ ¼ï¼‰
RULE_TYPE = "shishen"

# æ€§åˆ«æ˜ å°„
GENDER_MAP = {
    "æ— è®ºç”·å¥³": None,
    "ç”·": "male",
    "å¥³": "female",
}


def load_excel_rules(xlsx_path: str) -> Dict[str, List[Dict[str, Any]]]:
    """åŠ è½½Excelè§„åˆ™"""
    if not os.path.exists(xlsx_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {xlsx_path}")
    
    rules_by_sheet = {}
    xls = pd.ExcelFile(xlsx_path)
    
    print(f"ğŸ“– è¯»å–Excelæ–‡ä»¶: {xlsx_path}")
    print(f"   å·¥ä½œè¡¨æ•°é‡: {len(xls.sheet_names)}")
    print(f"   å·¥ä½œè¡¨åç§°: {', '.join(xls.sheet_names)}")
    
    for sheet_name in xls.sheet_names:
        df = pd.read_excel(xlsx_path, sheet_name=sheet_name)
        # æ¸…ç†ç©ºè¡Œ
        df = df.dropna(how='all')
        rows = df.to_dict("records")
        rules_by_sheet[sheet_name] = rows
        print(f"   - {sheet_name}: {len(rows)} æ¡è®°å½•")
    
    return rules_by_sheet


def generate_rule_code(rule_id: int, sheet_name: str = "åç¥å‘½æ ¼") -> str:
    """ç”Ÿæˆè§„åˆ™ç¼–ç """
    # ä½¿ç”¨ SHISHEN ä½œä¸ºå‰ç¼€ï¼Œé¿å…ä¸­æ–‡ç¼–ç é—®é¢˜
    return f"FORMULA_SHISHEN_{rule_id}"


def analyze_unparsed_rule(row: Dict[str, Any], reason: str) -> Dict[str, Any]:
    """åˆ†ææœªè§£æçš„è§„åˆ™ï¼Œç”Ÿæˆè¯¦ç»†è¯´æ˜"""
    rule_id = row.get("ID", 0)
    cond1 = str(row.get("ç­›é€‰æ¡ä»¶1", "")).strip()
    cond2 = str(row.get("ç­›é€‰æ¡ä»¶2", "")).strip()
    result = str(row.get("ç»“æœ", "")).strip()
    gender = str(row.get("æ€§åˆ«", "æ— è®ºç”·å¥³")).strip()
    qty = str(row.get("æ•°é‡", "")) if pd.notna(row.get("æ•°é‡")) else ""
    
    # åˆ†æä¸ç†è§£çš„ç‚¹
    unclear_points = []
    need_clarification = {}
    
    # æ£€æŸ¥æ¡ä»¶æ ¼å¼
    if not cond1 or not cond2:
        unclear_points.append("ç­›é€‰æ¡ä»¶ä¸ºç©ºæˆ–ä¸å®Œæ•´")
    
    # æ£€æŸ¥æ¡ä»¶ç±»å‹
    supported_cond1_types = ["æ—¥æŸ±", "æœˆæŸ±", "å¹´æŸ±", "æ—¶æŸ±", "åç¥", "ç¥ç…", "äº”è¡Œ", "æ—ºè¡°", "å››æŸ±", "æ—¥å¹²", "æœˆä»¤", "å¤©å¹²", "åœ°æ”¯", "ç¥ç…åç¥"]
    if cond1 and cond1 not in supported_cond1_types:
        unclear_points.append(f"ç­›é€‰æ¡ä»¶1ç±»å‹æœªçŸ¥: {cond1}")
        need_clarification[cond1] = f"éœ€è¦ç¡®è®¤'{cond1}'æ˜¯ä»€ä¹ˆç±»å‹çš„æ¡ä»¶"
    
    # æ£€æŸ¥æ¡ä»¶2çš„æ ¼å¼
    if cond2:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹²æ”¯
        import re
        ganzhi_pattern = r'[ç”²ä¹™ä¸™ä¸æˆŠå·±åºšè¾›å£¬ç™¸][å­ä¸‘å¯…å¯è¾°å·³åˆæœªç”³é…‰æˆŒäº¥]'
        ganzhi_matches = re.findall(ganzhi_pattern, cond2)
        if not ganzhi_matches and cond1 in ["æ—¥æŸ±", "æœˆæŸ±", "å¹´æŸ±", "æ—¶æŸ±"]:
            unclear_points.append(f"æ¡ä»¶2 '{cond2}' ä¸åŒ…å«æœ‰æ•ˆçš„å¹²æ”¯ç»„åˆ")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åç¥
        ten_gods = ["æ¯”è‚©", "åŠ«è´¢", "é£Ÿç¥", "ä¼¤å®˜", "æ­£è´¢", "åè´¢", "æ­£å®˜", "åå®˜", "ä¸ƒæ€", "æ­£å°", "åå°"]
        has_ten_god = any(tg in cond2 for tg in ten_gods)
        if not has_ten_god and cond1 == "åç¥":
            unclear_points.append(f"æ¡ä»¶2 '{cond2}' ä¸åŒ…å«æœ‰æ•ˆçš„åç¥åç§°")
    
    # æ­§ä¹‰è¯´æ˜
    ambiguity = ""
    if unclear_points:
        ambiguity = f"æ¡ä»¶æ ¼å¼ä¸æ˜ç¡®ï¼Œæ— æ³•ç¡®å®šå¦‚ä½•åŒ¹é…ã€‚å…·ä½“é—®é¢˜ï¼š{'; '.join(unclear_points)}"
    else:
        ambiguity = reason or "è§£æå™¨æ— æ³•è¯†åˆ«è¯¥æ¡ä»¶æ ¼å¼"
    
    # æ¡ˆä¾‹è¯´æ˜ï¼ˆå¦‚æœæœ‰ç»“æœæ–‡æœ¬ï¼‰
    case_example = {}
    if result:
        # å°è¯•ä»ç»“æœä¸­æå–å¯èƒ½çš„å‘½æ ¼åç§°
        case_example["ç»“æœæ–‡æœ¬"] = result[:200] + ("..." if len(result) > 200 else "")
    
    return {
        "ID": rule_id,
        "ç±»å‹": "åç¥å‘½æ ¼",
        "ç­›é€‰æ¡ä»¶1": cond1,
        "ç­›é€‰æ¡ä»¶2": cond2,
        "æ€§åˆ«": gender,
        "æ•°é‡": qty,
        "ç»“æœ": result[:500] + ("..." if len(result) > 500 else ""),  # é™åˆ¶é•¿åº¦
        "rule_code": generate_rule_code(rule_id),
        "è§£æå¤±è´¥åŸå› ": reason or "è§£æå¤±è´¥",
        "ä¸ç†è§£ç‚¹è¯´æ˜": {
            "ä¸ç†è§£çš„ç‚¹": unclear_points if unclear_points else ["è§£æå™¨æ— æ³•è¯†åˆ«è¯¥æ¡ä»¶æ ¼å¼"],
            "éœ€è¦æ¾„æ¸…çš„æ¦‚å¿µ": need_clarification,
            "æ­§ä¹‰è¯´æ˜": ambiguity,
            "æ¡ˆä¾‹è¯´æ˜": case_example,
            "è§£å†³æ–¹æ¡ˆ": "éœ€è¦æ‰©å±•è§£æå™¨æ”¯æŒè¯¥æ¡ä»¶æ ¼å¼ï¼Œæˆ–ç¡®è®¤æ¡ä»¶çš„æ­£ç¡®è¡¨è¾¾æ–¹å¼"
        }
    }


def import_rules(
    xlsx_path: str,
    write_db: bool = True,
    dry_run: bool = False,
) -> Tuple[List[RuleRecord], List[Dict[str, Any]], int, int, int]:
    """å¯¼å…¥è§„åˆ™
    
    Returns:
        (parsed_rules, unparsed_rules_details, inserted_count, updated_count, total_rules)
    """
    
    # åŠ è½½è§„åˆ™
    rules_by_sheet = load_excel_rules(xlsx_path)
    
    parsed_rules: List[RuleRecord] = []
    unparsed_rules: List[Dict[str, Any]] = []
    
    # è®¡ç®—æ€»è§„åˆ™æ•°
    total_rules = sum(len(rows) for rows in rules_by_sheet.values())
    
    # å¤„ç†æ‰€æœ‰å·¥ä½œè¡¨
    for sheet_name, rows in rules_by_sheet.items():
        print(f"\n{'='*60}")
        print(f"å¤„ç†å·¥ä½œè¡¨: {sheet_name}")
        print(f"{'='*60}")
        
        for idx, row in enumerate(rows, 1):
            rule_id = row.get("ID")
            if not rule_id or pd.isna(rule_id):
                print(f"  [{idx}/{len(rows)}] âš ï¸  è·³è¿‡ï¼šç¼ºå°‘ID")
                continue
            
            try:
                rule_id = int(rule_id)
            except (ValueError, TypeError):
                print(f"  [{idx}/{len(rows)}] âš ï¸  è·³è¿‡æ— æ•ˆID: {rule_id}")
                continue
            
            # è§£æè§„åˆ™
            try:
                result = RuleParser.parse(row, sheet_name)
            except Exception as e:
                print(f"  [{idx}/{len(rows)}] âŒ è§„åˆ™ {rule_id}: è§£æå¼‚å¸¸ - {e}")
                unparsed_detail = analyze_unparsed_rule(row, f"è§£æå¼‚å¸¸: {str(e)}")
                unparsed_rules.append(unparsed_detail)
                continue
            
            if not result.success:
                # åˆ†ææœªè§£æçš„è§„åˆ™
                unparsed_detail = analyze_unparsed_rule(row, result.reason or "è§£æå¤±è´¥")
                unparsed_rules.append(unparsed_detail)
                print(f"  [{idx}/{len(rows)}] âŒ è§„åˆ™ {rule_id}: {result.reason or 'è§£æå¤±è´¥'}")
                continue
            
            # æ„å»ºè§„åˆ™è®°å½•
            rule_code = generate_rule_code(rule_id, sheet_name)
            
            # æå–è§„åˆ™ç»“æœ
            result_text = str(row.get("ç»“æœ", "")).strip()
            if pd.isna(row.get("ç»“æœ")):
                result_text = ""
            
            rule_record = RuleRecord(
                rule_id=rule_id,
                rule_code=rule_code,
                rule_name=f"åç¥å‘½æ ¼è§„åˆ™-{rule_id}",
                rule_type=RULE_TYPE,  # ä½¿ç”¨ shishenï¼Œä¸æ˜¯ formula_shishen
                rule_category="shishen",
                priority=100,
                conditions=result.conditions,
                content={
                    "type": "text",
                    "text": result_text
                },
                description=json.dumps({
                    "ç­›é€‰æ¡ä»¶1": str(row.get("ç­›é€‰æ¡ä»¶1", "")),
                    "ç­›é€‰æ¡ä»¶2": str(row.get("ç­›é€‰æ¡ä»¶2", "")),
                    "æ€§åˆ«": str(row.get("æ€§åˆ«", "æ— è®ºç”·å¥³")),
                    "æ•°é‡": str(row.get("æ•°é‡", "")) if pd.notna(row.get("æ•°é‡")) else "",
                }, ensure_ascii=False),
                source=sheet_name
            )
            parsed_rules.append(rule_record)
            print(f"  [{idx}/{len(rows)}] âœ… è§„åˆ™ {rule_id}: è§£ææˆåŠŸ")
    
    # å†™å…¥æ•°æ®åº“
    inserted_count = 0
    updated_count = 0
    
    if write_db and parsed_rules and not dry_run:
        from server.config.mysql_config import get_mysql_connection, return_mysql_connection
        import time
        
        # é‡è¯•è¿æ¥ï¼ˆæœ€å¤š3æ¬¡ï¼‰
        conn = None
        for retry in range(3):
            try:
                conn = get_mysql_connection()
                break
            except Exception as e:
                if retry < 2:
                    print(f"  âš ï¸  è¿æ¥å¤±è´¥ï¼Œç­‰å¾…5ç§’åé‡è¯• ({retry+1}/3)...")
                    time.sleep(5)
                else:
                    raise
        
        if not conn:
            raise Exception("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“")
        
        try:
            with conn.cursor() as cur:
                # è·å–å·²å­˜åœ¨çš„è§„åˆ™
                existing_codes = set()
                cur.execute("SELECT rule_code FROM bazi_rules WHERE rule_code LIKE 'FORMULA_SHISHEN_%'")
                existing_codes = {item["rule_code"] for item in cur.fetchall()}
                
                insert_sql = """
                    INSERT INTO bazi_rules 
                    (rule_code, rule_name, rule_type, rule_category, priority, conditions, content, description, enabled)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                """
                
                update_sql = """
                    UPDATE bazi_rules SET
                        rule_name = %s,
                        rule_type = %s,
                        rule_category = %s,
                        priority = %s,
                        conditions = %s,
                        content = %s,
                        description = %s,
                        updated_at = NOW()
                    WHERE rule_code = %s
                """
                
                for rule in parsed_rules:
                    if rule.rule_code in existing_codes:
                        # æ›´æ–°
                        cur.execute(update_sql, (
                            rule.rule_name,
                            rule.rule_type,
                            rule.rule_category,
                            rule.priority,
                            json.dumps(rule.conditions, ensure_ascii=False),
                            json.dumps(rule.content, ensure_ascii=False),
                            rule.description,
                            rule.rule_code
                        ))
                        updated_count += 1
                    else:
                        # æ’å…¥
                        cur.execute(insert_sql, (
                            rule.rule_code,
                            rule.rule_name,
                            rule.rule_type,
                            rule.rule_category,
                            rule.priority,
                            json.dumps(rule.conditions, ensure_ascii=False),
                            json.dumps(rule.content, ensure_ascii=False),
                            rule.description,
                            1  # enabled
                        ))
                        inserted_count += 1
                
                conn.commit()
                print(f"\nâœ… æ•°æ®åº“æ“ä½œå®Œæˆ: æ–°å¢ {inserted_count} æ¡, æ›´æ–° {updated_count} æ¡")
        except Exception as e:
            conn.rollback()
            print(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
            raise
        finally:
            return_mysql_connection(conn)
    elif dry_run or (not write_db):
        # é¢„è§ˆæ¨¡å¼ï¼šä¸è¿æ¥æ•°æ®åº“ï¼Œåªç»Ÿè®¡
        print(f"\nâš ï¸  é¢„è§ˆæ¨¡å¼: æˆåŠŸè§£æ {len(parsed_rules)} æ¡è§„åˆ™")
        print(f"   æ³¨æ„: å®é™…å¯¼å…¥æ—¶ä¼šæ£€æŸ¥ç°æœ‰è§„åˆ™ï¼Œå¯èƒ½éƒ¨åˆ†è§„åˆ™ä¼šæ›´æ–°è€Œéæ–°å¢")
    
    return parsed_rules, unparsed_rules, inserted_count, updated_count, total_rules


def save_unparsed_rules(unparsed_rules: List[Dict[str, Any]], output_path: str, total_rules: int):
    """ä¿å­˜æœªè§£æè§„åˆ™åˆ°JSONæ–‡ä»¶"""
    # ç»Ÿè®¡ä¿¡æ¯
    parsed_count = total_rules - len(unparsed_rules)
    stats = {
        "æ€»è§„åˆ™æ•°": total_rules,
        "æˆåŠŸè§£æ": parsed_count,
        "æ— æ³•è§£æ": len(unparsed_rules),
        "è§£ææˆåŠŸç‡": f"{parsed_count / total_rules * 100:.1f}%" if total_rules > 0 else "0%"
    }
    
    output_data = {
        "ç»Ÿè®¡": stats,
        "æœªè§£æè§„åˆ™è¯¦ç»†è¯´æ˜": unparsed_rules,
        "ç”Ÿæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ æœªè§£æè§„åˆ™å·²ä¿å­˜åˆ°: {output_path}")
    print(f"   æ— æ³•è§£æ: {len(unparsed_rules)} æ¡")


def escape_sql_string(s: str) -> str:
    """è½¬ä¹‰SQLå­—ç¬¦ä¸²"""
    if not s:
        return "''"
    # è½¬ä¹‰å•å¼•å·å’Œåæ–œæ 
    s = s.replace('\\', '\\\\').replace("'", "\\'")
    return f"'{s}'"


def generate_sql_file(parsed_rules: List[RuleRecord], sql_path: str):
    """ç”ŸæˆSQLæ–‡ä»¶"""
    sql_statements = []
    sql_statements.append("-- å¯¼å…¥åç¥å‘½æ ¼12.20.xlsxè§„åˆ™")
    sql_statements.append("USE hifate_bazi;")
    sql_statements.append("")
    
    for rule in parsed_rules:
        # è½¬ä¹‰JSONå­—ç¬¦ä¸²
        conditions_json = json.dumps(rule.conditions, ensure_ascii=False)
        content_json = json.dumps(rule.content, ensure_ascii=False)
        description_json = rule.description  # å·²ç»æ˜¯JSONå­—ç¬¦ä¸²
        
        sql_statements.append(f"-- è§„åˆ™ {rule.rule_id}: {rule.rule_name}")
        sql_statements.append("INSERT INTO bazi_rules (rule_code, rule_name, rule_type, rule_category, priority, conditions, content, description, enabled)")
        sql_statements.append(f"VALUES ({escape_sql_string(rule.rule_code)}, {escape_sql_string(rule.rule_name)}, {escape_sql_string(rule.rule_type)}, {escape_sql_string(rule.rule_category)}, {rule.priority}, {escape_sql_string(conditions_json)}, {escape_sql_string(content_json)}, {escape_sql_string(description_json)}, 1)")
        sql_statements.append("ON DUPLICATE KEY UPDATE")
        sql_statements.append(f"  rule_name = {escape_sql_string(rule.rule_name)},")
        sql_statements.append(f"  rule_type = {escape_sql_string(rule.rule_type)},")
        sql_statements.append(f"  rule_category = {escape_sql_string(rule.rule_category)},")
        sql_statements.append(f"  priority = {rule.priority},")
        sql_statements.append(f"  conditions = {escape_sql_string(conditions_json)},")
        sql_statements.append(f"  content = {escape_sql_string(content_json)},")
        sql_statements.append(f"  description = {escape_sql_string(description_json)},")
        sql_statements.append("  updated_at = NOW();")
        sql_statements.append("")
    
    with open(sql_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(sql_statements))
    
    print(f"\nğŸ“ SQLæ–‡ä»¶å·²ç”Ÿæˆ: {sql_path}")


def main():
    parser = argparse.ArgumentParser(description='å¯¼å…¥åç¥å‘½æ ¼12.20.xlsxè§„åˆ™åˆ°æ•°æ®åº“')
    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼ï¼Œä¸å†™å…¥æ•°æ®åº“')
    parser.add_argument('--xlsx', type=str, default=XLSX_FILE, help='Excelæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default=OUTPUT_JSON, help='æœªè§£æè§„åˆ™JSONè¾“å‡ºè·¯å¾„')
    parser.add_argument('--sql-only', action='store_true', help='åªç”ŸæˆSQLæ–‡ä»¶ï¼Œä¸å¯¼å…¥æ•°æ®åº“')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ“¥ å¯¼å…¥åç¥å‘½æ ¼12.20.xlsxè§„åˆ™")
    print("=" * 60)
    
    if args.dry_run:
        print("\nâš ï¸  é¢„è§ˆæ¨¡å¼ï¼ˆä¸å†™å…¥æ•°æ®åº“ï¼‰")
    
    try:
        # å¯¼å…¥è§„åˆ™
        parsed_rules, unparsed_rules, inserted_count, updated_count, total_rules = import_rules(
            args.xlsx,
            write_db=not args.sql_only and not args.dry_run,  # sql-onlyæ¨¡å¼ä¸å†™å…¥æ•°æ®åº“
            dry_run=args.dry_run or args.sql_only  # sql-onlyæ¨¡å¼è§†ä¸ºdry-run
        )
        
        # ä¿å­˜æœªè§£æè§„åˆ™
        if unparsed_rules:
            save_unparsed_rules(unparsed_rules, args.output, total_rules)
        
        # è¾“å‡ºç»Ÿè®¡
        total_parsed = len(parsed_rules)
        total_unparsed = len(unparsed_rules)
        
        print("\n" + "=" * 60)
        print("ğŸ“Š å¯¼å…¥ç»Ÿè®¡")
        print("=" * 60)
        print(f"æ€»è§„åˆ™æ•°: {total_rules}")
        print(f"æˆåŠŸè§£æ: {total_parsed} ({total_parsed/total_rules*100:.1f}%)" if total_rules > 0 else "æˆåŠŸè§£æ: 0")
        print(f"æ— æ³•è§£æ: {total_unparsed} ({total_unparsed/total_rules*100:.1f}%)" if total_rules > 0 else "æ— æ³•è§£æ: 0")
        
        if not args.dry_run and not args.sql_only:
            print(f"\næ•°æ®åº“æ“ä½œ:")
            print(f"  æ–°å¢: {inserted_count} æ¡")
            print(f"  æ›´æ–°: {updated_count} æ¡")
        
        # å¦‚æœåªç”ŸæˆSQLæ–‡ä»¶
        if args.sql_only and parsed_rules:
            sql_path = os.path.join(PROJECT_ROOT, "docs", "import_shishen_12_20_rules.sql")
            generate_sql_file(parsed_rules, sql_path)
        
        print("\nâœ… å¯¼å…¥å®Œæˆ!")
        
        # å¦‚æœæœ‰æœªè§£æçš„è§„åˆ™ï¼Œæç¤ºç”¨æˆ·
        if unparsed_rules:
            print(f"\nâš ï¸  å‘ç° {len(unparsed_rules)} æ¡æ— æ³•è§£æçš„è§„åˆ™")
            print(f"   è¯¦ç»†ä¿¡æ¯å·²ä¿å­˜åˆ°: {args.output}")
            print(f"   è¯·æŸ¥çœ‹æ–‡ä»¶å¹¶å‘Šè¯‰æˆ‘å¦‚ä½•å¤„ç†è¿™äº›è§„åˆ™")
        
    except Exception as e:
        print(f"\nâŒ å¯¼å…¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

