#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ¹æ® docs/æ¡ƒèŠ±ç®—æ³•å…¬å¼.json è§£ææ¡ƒèŠ±ã€æ­£ç¼˜ã€å©šå§»ã€å¯Œè´µå‘½è§„åˆ™ï¼Œå¹¶æ‰¹é‡å†™å…¥ MySQLã€‚
æ”¯æŒ --dry-run ä»…æ‰“å°è§£æç»“æœè€Œä¸å†™æ•°æ®åº“ã€‚
"""

from __future__ import annotations

import argparse
import json
import os
import re
import sys
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Set, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

# å¯¼å…¥å©šå§»è§„åˆ™çš„è§£æå‡½æ•°
from scripts.import_marriage_rules import (
    CONDITION_HANDLERS,
    normalize_gender,
    build_conditions,
    load_pending_rule_ids,
    resolve_json_path,
    RULE_TYPE_MAP,
    PILLAR_NAMES,
    make_pillar_equals,
    make_pillar_in,
)

DEFAULT_JSON_PATH = os.path.join(PROJECT_ROOT, "docs", "æ¡ƒèŠ±ç®—æ³•å…¬å¼.json")
DEFAULT_PENDING_PATH = os.path.join(PROJECT_ROOT, "docs", "taohua_rule_pending_confirmation.json")

# è§„åˆ™ç±»å‹æ˜ å°„ï¼ˆåŸºäºç±»å‹å­—æ®µï¼‰
RULE_CATEGORY_MAP = {
    "æ¡ƒèŠ±": "taohua",
    "æ­£ç¼˜": "zhengyuan",
    "å©šå§»": "marriage",
    "ä¸Šä¸Šä¹˜å¯Œè´µå‘½": "fugui",
}

# è§„åˆ™ä»£ç å‰ç¼€æ˜ å°„
RULE_CODE_PREFIX_MAP = {
    "æ¡ƒèŠ±": "TAOHUA",
    "æ­£ç¼˜": "ZHENGYUAN",
    "å©šå§»": "MARRIAGE",
    "ä¸Šä¸Šä¹˜å¯Œè´µå‘½": "FUGUI",
}


@dataclass
class ParsedRule:
    rule: Dict[str, Any]
    row: Dict[str, Any]
    source: str
    sheet: str


@dataclass
class SkippedRule:
    rule_id: int
    reason: str
    source: str
    sheet: str
    rule_type: str = ""


def load_rows(json_path: str) -> List[Dict[str, Any]]:
    """ä»æ¡ƒèŠ±ç®—æ³•å…¬å¼.jsonåŠ è½½æ•°æ®"""
    path = resolve_json_path(json_path)
    if not os.path.exists(path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è§„åˆ™è¡¨: {path}")
    
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    raw_rows: List[Dict[str, Any]] = []
    sheets = data.get("sheets", {})
    
    for sheet_name, sheet_data in sheets.items():
        rows = sheet_data.get("data", [])
        for row in rows:
            if not row or not isinstance(row, dict):
                continue
            record = dict(row)
            record["_source"] = os.path.basename(path)
            record["_sheet"] = sheet_name
            raw_rows.append(record)
    
    return raw_rows


def handle_taohua_nayin(cond2: str, qty: str) -> Tuple[Optional[List[Dict[str, Any]]], Optional[str]]:
    """å¤„ç†æ¡ƒèŠ±çº³éŸ³æ¡ä»¶ï¼Œæ”¯æŒå¤§è¿/æµå¹´"""
    # ç¤ºä¾‹ï¼šæ—¥æŸ±çš„çº³éŸ³äº”è¡Œå±æ€§ä¸ºé‡‘çš„ï¼Œå¹´ã€æœˆã€æ—¥ã€æ—¶æŸ±æˆ–å¤§è¿ã€æµå¹´ä¸­ï¼Œè§åˆ°"å·³"æˆ–"äº¥"åœ°æ”¯
    # ä½¿ç”¨æ›´çµæ´»çš„æ¨¡å¼åŒ¹é…ä¸­æ–‡å¼•å·ï¼ˆä¸­æ–‡å¼•å·æ˜¯ "" (U+201C) å’Œ "" (U+201D)ï¼‰
    # ä½¿ç”¨ Unicode è½¬ä¹‰æˆ–å­—ç¬¦ç±»æ¥åŒ¹é…ä¸­æ–‡å¼•å·
    pattern = r'æ—¥æŸ±çš„çº³éŸ³äº”è¡Œå±æ€§ä¸º([æœ¨ç«åœŸé‡‘æ°´])çš„.*?è§åˆ°[\u201C\u201D"]([^\u201C\u201D"]+)[\u201C\u201D"].*?[\u201C\u201D"]([^\u201C\u201D"]+)[\u201C\u201D"]åœ°æ”¯'
    match = re.search(pattern, cond2)
    if match:
        element = match.group(1)
        branch1 = match.group(2)
        branch2 = match.group(3)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤§è¿/æµå¹´
        has_dayun_liunian = "å¤§è¿" in cond2 or "æµå¹´" in cond2
        
        # æ„å»ºæ¡ä»¶ï¼šæ—¥æŸ±çº³éŸ³ + åœ¨ä»»æ„æŸ±/å¤§è¿/æµå¹´ä¸­æŸ¥æ‰¾æŒ‡å®šåœ°æ”¯
        conditions = [
            {"pillar_element": {"pillar": "day", "part": "nayin", "in": [element]}}
        ]
        
        # åœ¨å››æŸ±ä¸­æŸ¥æ‰¾
        pillar_conditions = [
            make_pillar_in(pillar, "branch", [branch1, branch2])
            for pillar in PILLAR_NAMES
        ]
        
        # å¦‚æœåŒ…å«å¤§è¿/æµå¹´ï¼Œæ·»åŠ å¤§è¿/æµå¹´æ¡ä»¶
        if has_dayun_liunian:
            # å¤§è¿æ¡ä»¶ï¼šåœ¨å¤§è¿åœ°æ”¯ä¸­æŸ¥æ‰¾
            dayun_condition = {
                "dayun_branch_in": {
                    "values": [branch1, branch2]
                }
            }
            # æµå¹´æ¡ä»¶ï¼šåœ¨æµå¹´åœ°æ”¯ä¸­æŸ¥æ‰¾
            liunian_condition = {
                "liunian_branch_in": {
                    "values": [branch1, branch2]
                }
            }
            # ä»»æ„ä¸€ä¸ªæ»¡è¶³å³å¯ï¼ˆå››æŸ±ã€å¤§è¿ã€æµå¹´ï¼‰
            conditions.append({
                "any": pillar_conditions + [dayun_condition, liunian_condition]
            })
        else:
            # åªåœ¨å››æŸ±ä¸­æŸ¥æ‰¾
            conditions.append({"any": pillar_conditions})
        
        return conditions, None
    return None, f"æœªå®ç°çš„çº³éŸ³æ¡ƒèŠ±æ¡ä»¶: {cond2}"


def handle_taohua_pillar_combination(cond2: str, qty: str) -> Tuple[Optional[List[Dict[str, Any]]], Optional[str]]:
    """å¤„ç†æ¡ƒèŠ±æ—¥æŸ±ç»„åˆæ¡ä»¶ï¼Œå¦‚ï¼šæ—¥æŸ±ä¸ºä¸™å­ï¼Œæ—¶æŸ±è¾›å¯"""
    # åŒ¹é…ï¼šæ—¥æŸ±ä¸ºXXXï¼Œæ—¶æŸ±YYY
    pattern = r"æ—¥æŸ±ä¸º([^ï¼Œ,]+)ï¼Œæ—¶æŸ±([^ï¼Œ,]+)"
    match = re.search(pattern, cond2)
    if match:
        day_pillar = match.group(1).strip()
        hour_pillar = match.group(2).strip()
        return [
            make_pillar_equals("day", [day_pillar]),
            make_pillar_equals("hour", [hour_pillar]),
        ], None
    
    # åŒ¹é…ï¼šæ—¶æŸ±ä¸ºXXXï¼Œæ—¥æŸ±YYY
    pattern = r"æ—¶æŸ±ä¸º([^ï¼Œ,]+)ï¼Œæ—¥æŸ±([^ï¼Œ,]+)"
    match = re.search(pattern, cond2)
    if match:
        hour_pillar = match.group(1).strip()
        day_pillar = match.group(2).strip()
        return [
            make_pillar_equals("day", [day_pillar]),
            make_pillar_equals("hour", [hour_pillar]),
        ], None
    
    return None, f"æœªå®ç°çš„æ—¥æŸ±ç»„åˆæ¡ä»¶: {cond2}"


def handle_taohua_day_branch_complex(cond2: str, qty: str) -> Tuple[Optional[List[Dict[str, Any]]], Optional[str]]:
    """å¤„ç†å¤æ‚çš„æ—¥æ”¯æ¡ä»¶ï¼Œå¦‚ï¼šæ—¥æ”¯æ˜¯ç”³ã€å­ã€è¾°å…¶ä¸­ä¹‹ä¸€ï¼Œä¸”æ—¶æ”¯æ˜¯é…‰
    æ”¯æŒå†œå†æœˆä»½æ¡ä»¶ï¼Œå¦‚ï¼šæ—¥æ”¯æ˜¯ç”³ã€å­ã€è¾°å…¶ä¸­ä¹‹ä¸€ï¼Œå†œå†æ˜¯4ã€5ã€6æœˆä»½å…¶ä¸­ä¹‹ä¸€å‡ºç”Ÿï¼Œä¸”æ—¶æ”¯æ˜¯å·³"""
    # åŒ¹é…ï¼šæ—¥æ”¯æ˜¯XXXã€YYYã€ZZZå…¶ä¸­ä¹‹ä¸€ï¼Œä¸”æ—¶æ”¯æ˜¯WWWï¼ˆå¯èƒ½ä¸­é—´æœ‰å†œå†æœˆä»½æ¡ä»¶ï¼‰
    # ä½¿ç”¨æ›´çµæ´»çš„æ¨¡å¼ï¼Œå…è®¸ä¸­é—´æœ‰å†œå†æœˆä»½æ¡ä»¶
    pattern = r"æ—¥æ”¯æ˜¯([^ï¼Œ,]+?)(?:ï¼Œ.*?)?ä¸”æ—¶æ”¯æ˜¯([^ï¼Œ,]+)"
    match = re.search(pattern, cond2)
    if match:
        day_branches_str = match.group(1).strip()
        hour_branch = match.group(2).strip()
        
        # æå–æ—¥æ”¯åˆ—è¡¨ï¼ˆå¯èƒ½æ˜¯"ç”³ã€å­ã€è¾°å…¶ä¸­ä¹‹ä¸€"æˆ–ç›´æ¥æ˜¯"ç”³ã€å­ã€è¾°"ï¼‰
        day_branches = [b.strip() for b in re.split(r"[ã€,ï¼Œ]", day_branches_str) if b.strip()]
        # ç§»é™¤"å…¶ä¸­ä¹‹ä¸€"ç­‰åç¼€
        day_branches = [b.replace("å…¶ä¸­ä¹‹ä¸€", "").strip() for b in day_branches if b.replace("å…¶ä¸­ä¹‹ä¸€", "").strip()]
        
        conditions = []
        if day_branches:
            conditions.append(make_pillar_in("day", "branch", day_branches))
        if hour_branch:
            conditions.append(make_pillar_in("hour", "branch", [hour_branch]))
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å†œå†æœˆä»½æ¡ä»¶
        lunar_month_pattern = r"å†œå†æ˜¯([^ï¼Œ,]+)æœˆä»½"
        lunar_match = re.search(lunar_month_pattern, cond2)
        if lunar_match:
            months_str = lunar_match.group(1).strip()
            # æå–æœˆä»½æ•°å­—
            months = []
            for m in re.split(r"[ã€,ï¼Œ]", months_str):
                m = m.strip()
                # æå–æ•°å­—
                numbers = re.findall(r"\d+", m)
                if numbers:
                    months.extend([int(n) for n in numbers])
            
            if months:
                conditions.append({
                    "lunar_month_in": {
                        "values": months
                    }
                })
        
        if conditions:
            return conditions, None
    
    return None, f"æœªå®ç°çš„å¤æ‚æ—¥æ”¯æ¡ä»¶: {cond2}"


def handle_taohua_simple_branches(cond2: str, qty: str) -> Tuple[Optional[List[Dict[str, Any]]], Optional[str]]:
    """å¤„ç†ç®€å•çš„åœ°æ”¯åˆ—è¡¨ï¼Œå¦‚ï¼šå­ã€åˆã€å¯ã€é…‰"""
    branches = [b.strip() for b in re.split(r"[ã€,ï¼Œ]", cond2) if b.strip() and b.strip() in "å­ä¸‘å¯…å¯è¾°å·³åˆæœªç”³é…‰æˆŒäº¥"]
    if branches:
        return [make_pillar_in("day", "branch", branches)], None
    return None, f"æ— æ³•è§£æåœ°æ”¯åˆ—è¡¨: {cond2}"


# æ‰©å±•æ¡ä»¶å¤„ç†å™¨
EXTENDED_CONDITION_HANDLERS = CONDITION_HANDLERS.copy()
EXTENDED_CONDITION_HANDLERS["çº³éŸ³"] = handle_taohua_nayin


def build_taohua_conditions(row: Dict[str, Any]) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
    """æ„å»ºæ¡ƒèŠ±è§„åˆ™æ¡ä»¶"""
    rule_id = int(row["ID"])
    cond1 = (row.get("ç­›é€‰æ¡ä»¶1") or "").strip()
    cond2 = (row.get("ç­›é€‰æ¡ä»¶2") or "").strip()
    qty = (row.get("æ•°é‡") or "").strip()
    gender = normalize_gender(row.get("æ€§åˆ«"))
    rule_type = row.get("ç±»å‹", "").strip()

    conds: List[Dict[str, Any]] = []
    if gender:
        conds.append({"gender": gender})

    if not cond1:
        return None, "ç¼ºå°‘ç­›é€‰æ¡ä»¶1"

    if not cond2:
        return None, "ç¼ºå°‘ç­›é€‰æ¡ä»¶2"

    # ç‰¹æ®Šå¤„ç†æ¡ƒèŠ±ç›¸å…³çš„å¤æ‚æ¡ä»¶
    if rule_type == "æ¡ƒèŠ±" and cond1 == "çº³éŸ³":
        extra_conditions, reason = handle_taohua_nayin(cond2, qty)
    elif rule_type == "æ¡ƒèŠ±" and cond1 == "æ—¥æŸ±" and ("æ—¶æŸ±" in cond2 or "æ—¥æŸ±ä¸º" in cond2):
        extra_conditions, reason = handle_taohua_pillar_combination(cond2, qty)
    elif cond1 == "æ—¥æ”¯" and ("ä¸”æ—¶æ”¯" in cond2 or "å†œå†" in cond2):
        # æ”¯æŒå†œå†æœˆä»½æ¡ä»¶çš„å¤æ‚æ—¥æ”¯æ¡ä»¶
        extra_conditions, reason = handle_taohua_day_branch_complex(cond2, qty)
    elif cond1 in ["æ—¥æ”¯", "æ—¶æ”¯"] and re.match(r"^[å­ä¸‘å¯…å¯è¾°å·³åˆæœªç”³é…‰æˆŒäº¥ã€ï¼Œ,]+$", cond2):
        # ç®€å•çš„åœ°æ”¯åˆ—è¡¨
        if cond1 == "æ—¥æ”¯":
            branches = [b.strip() for b in re.split(r"[ã€,ï¼Œ]", cond2) if b.strip() and b.strip() in "å­ä¸‘å¯…å¯è¾°å·³åˆæœªç”³é…‰æˆŒäº¥"]
            if branches:
                extra_conditions, reason = [make_pillar_in("day", "branch", branches)], None
            else:
                extra_conditions, reason = None, f"æ— æ³•è§£ææ—¥æ”¯åˆ—è¡¨: {cond2}"
        elif cond1 == "æ—¶æ”¯":
            branches = [b.strip() for b in re.split(r"[ã€,ï¼Œ]", cond2) if b.strip() and b.strip() in "å­ä¸‘å¯…å¯è¾°å·³åˆæœªç”³é…‰æˆŒäº¥"]
            if branches:
                extra_conditions, reason = [make_pillar_in("hour", "branch", branches)], None
            else:
                extra_conditions, reason = None, f"æ— æ³•è§£ææ—¶æ”¯åˆ—è¡¨: {cond2}"
        else:
            extra_conditions, reason = None, f"æœªå¤„ç†çš„ç®€å•åœ°æ”¯æ¡ä»¶: {cond1}"
    else:
        # ä½¿ç”¨æ ‡å‡†å¤„ç†å™¨
        try:
            handler = EXTENDED_CONDITION_HANDLERS.get(cond1)
            if handler:
                extra_conditions, reason = handler(cond2, qty)
            else:
                return None, f"æš‚æœªæ”¯æŒçš„ç­›é€‰æ¡ä»¶ç±»å‹:{cond1}"
        except Exception as exc:
            return None, f"è§£æå¼‚å¸¸: {exc}"

    if extra_conditions is None:
        return None, reason or "æ— æ³•è§£ææ¡ä»¶"

    conds.extend(extra_conditions)
    conds = [c for c in conds if c]
    if not conds:
        return None, reason or "æœªç”Ÿæˆä»»ä½•æ¡ä»¶"

    if len(conds) == 1:
        return conds[0], None
    return {"all": conds}, None


def import_rules(
    json_path: str,
    write_db: bool = True,
    append: bool = False,
    pending_path: Optional[str] = DEFAULT_PENDING_PATH,
) -> Tuple[List[ParsedRule], List[SkippedRule], List[SkippedRule], int, int]:
    """å¯¼å…¥è§„åˆ™"""
    rows = load_rows(json_path)
    pending_ids = load_pending_rule_ids(pending_path)
    parsed: List[ParsedRule] = []
    skipped: List[SkippedRule] = []
    skipped_existing: List[SkippedRule] = []
    seen_codes: Set[str] = set()

    for row in rows:
        source = row.get("_source", "")
        sheet = row.get("_sheet", "")
        rule_type = row.get("ç±»å‹", "").strip()
        raw_rule_id = str(row.get("ID", "")).strip()
        
        try:
            rule_id = int(raw_rule_id)
        except (TypeError, ValueError):
            skipped.append(SkippedRule(
                rule_id=-1,
                reason="ID ç¼ºå¤±æˆ–éæ³•",
                source=source,
                sheet=sheet,
                rule_type=rule_type
            ))
            continue

        if pending_ids and rule_id in pending_ids:
            skipped.append(SkippedRule(
                rule_id=rule_id,
                reason="å¾…ç¡®è®¤è§„åˆ™ï¼Œæš‚ä¸å¯¼å…¥",
                source=source,
                sheet=sheet,
                rule_type=rule_type
            ))
            continue

        condition, reason = build_taohua_conditions(row)
        if not condition:
            skipped.append(SkippedRule(
                rule_id=rule_id,
                reason=reason or "æœªè§£æ",
                source=source,
                sheet=sheet,
                rule_type=rule_type
            ))
            continue

        # ç”Ÿæˆè§„åˆ™ä»£ç 
        prefix = RULE_CODE_PREFIX_MAP.get(rule_type, "TAOHUA")
        rule_code = f"{prefix}-{rule_id}"
        
        if rule_code in seen_codes:
            skipped.append(SkippedRule(
                rule_id=rule_id,
                reason="rule_code é‡å¤ï¼ˆè¾“å…¥æ–‡ä»¶ï¼‰",
                source=source,
                sheet=sheet,
                rule_type=rule_type
            ))
            continue
        seen_codes.add(rule_code)

        cond1 = (row.get("ç­›é€‰æ¡ä»¶1") or "").strip()
        rule_type_db = RULE_TYPE_MAP.get(cond1, f"{RULE_CATEGORY_MAP.get(rule_type, 'taohua')}_general")
        rule_category = RULE_CATEGORY_MAP.get(rule_type, "taohua")
        
        rule_dict = {
            "rule_code": rule_code,
            "rule_name": f"{rule_type}è§„åˆ™{rule_id}",
            "rule_type": rule_type_db,
            "rule_category": rule_category,
            "priority": 85,
            "conditions": condition,
            "content": {
                "type": "description",
                "text": (row.get("ç»“æœ") or "").strip(),
            },
        }
        parsed.append(ParsedRule(rule=rule_dict, row=row, source=source, sheet=sheet))

    inserted_count = 0
    updated_count = 0

    if write_db and parsed:
        from server.config.mysql_config import get_mysql_connection  # noqa: E402

        conn = get_mysql_connection()
        try:
            with conn.cursor() as cur:
                existing_codes: Set[str] = set()
                if append:
                    # æŸ¥è¯¢æ‰€æœ‰ç›¸å…³å‰ç¼€çš„è§„åˆ™ï¼ˆMySQLè¯­æ³•ï¼‰
                    prefixes = list(RULE_CODE_PREFIX_MAP.values())
                    if prefixes:
                        conditions = " OR ".join(["rule_code LIKE %s"] * len(prefixes))
                        patterns = [f"{prefix}-%" for prefix in prefixes]
                        cur.execute(f"SELECT rule_code FROM bazi_rules WHERE {conditions}", tuple(patterns))
                        existing_codes = {item["rule_code"] for item in cur.fetchall()}
                else:
                    # åªåˆ é™¤å½“å‰ç±»å‹çš„è§„åˆ™
                    for prefix in RULE_CODE_PREFIX_MAP.values():
                        cur.execute("DELETE FROM bazi_rules WHERE rule_code LIKE %s", (f"{prefix}-%",))

                sql = (
                    "INSERT INTO bazi_rules "
                    "(rule_code, rule_name, rule_type, rule_category, priority, conditions, content, description, enabled)"
                    " VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s)"
                )

                values = []
                update_values = []
                for item in parsed:
                    rule = item.rule
                    code = rule["rule_code"]
                    if append and code in existing_codes:
                        update_values.append((
                            rule["rule_name"],
                            rule["rule_type"],
                            rule["rule_category"],
                            rule["priority"],
                            json.dumps(rule["conditions"], ensure_ascii=False),
                            json.dumps(rule["content"], ensure_ascii=False),
                            rule["content"]["text"],
                            True,
                            rule["rule_code"],
                        ))
                    else:
                        values.append((
                            rule["rule_code"],
                            rule["rule_name"],
                            rule["rule_type"],
                            rule["rule_category"],
                            rule["priority"],
                            json.dumps(rule["conditions"], ensure_ascii=False),
                            json.dumps(rule["content"], ensure_ascii=False),
                            rule["content"]["text"],
                            True,
                        ))

                if values:
                    cur.executemany(sql, values)
                    inserted_count = len(values)

                if update_values:
                    update_sql = """
                        UPDATE bazi_rules
                        SET rule_name = %s, rule_type = %s, rule_category = %s, priority = %s,
                            conditions = %s, content = %s, description = %s, enabled = %s
                        WHERE rule_code = %s
                    """
                    cur.executemany(update_sql, update_values)
                    updated_count = len(update_values)

                if values or update_values:
                    cur.execute("UPDATE rule_version SET rule_version = rule_version + 1, content_version = content_version + 1")
                conn.commit()
        except Exception:
            conn.rollback()
            raise
        finally:
            conn.close()

    return parsed, skipped, skipped_existing, inserted_count, updated_count


def save_pending_rules(skipped: List[SkippedRule], output_path: str):
    """ä¿å­˜å¾…ç¡®è®¤è§„åˆ™åˆ°JSONæ–‡ä»¶"""
    pending_rules = []
    for item in skipped:
        pending_rules.append({
            "id": item.rule_id,
            "rule_type": item.rule_type,
            "sheet": item.sheet,
            "source": item.source,
            "reason": item.reason,
        })
    
    output = {
        "rules_requiring_confirmation": pending_rules,
        "total_count": len(pending_rules),
    }
    
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ å¾…ç¡®è®¤è§„åˆ™å·²ä¿å­˜åˆ°: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="å¯¼å…¥æ¡ƒèŠ±ç®—æ³•è§„åˆ™åˆ°æ•°æ®åº“")
    parser.add_argument("--dry-run", action="store_true", help="ä»…è§£æå¹¶æ‰“å°ç»“æœï¼Œä¸å†™å…¥æ•°æ®åº“")
    parser.add_argument("--append", action="store_true", help="é‡‡ç”¨è¿½åŠ æ¨¡å¼ï¼Œä¸æ¸…ç©ºç°æœ‰è§„åˆ™")
    parser.add_argument("--json-path", dest="json_path", default=DEFAULT_JSON_PATH,
                        help=f"æŒ‡å®šè¦è§£æçš„ JSON æ–‡ä»¶ï¼ˆé»˜è®¤ {DEFAULT_JSON_PATH}ï¼‰")
    parser.add_argument("--pending-json", dest="pending_json", default=DEFAULT_PENDING_PATH,
                        help=f"å¾…ç¡®è®¤è§„åˆ™åˆ—è¡¨ JSONï¼ˆé»˜è®¤ {DEFAULT_PENDING_PATH}ï¼‰")
    args = parser.parse_args()

    parsed, skipped, skipped_existing, inserted, updated = import_rules(
        json_path=args.json_path,
        write_db=not args.dry_run,
        append=args.append,
        pending_path=args.pending_json,
    )

    print(f"âœ“ å¯è§£æè§„åˆ™: {len(parsed)} æ¡, å¾…ç¡®è®¤/è·³è¿‡: {len(skipped)} æ¡, å·²å­˜åœ¨è·³è¿‡: {len(skipped_existing)} æ¡")
    print(f"ä½¿ç”¨ JSON æ–‡ä»¶: {args.json_path}")
    
    if not args.dry_run:
        if inserted > 0:
            print(f"æ•°æ®åº“å®é™…æ–°å¢è§„åˆ™: {inserted} æ¡")
        if updated > 0:
            print(f"æ•°æ®åº“å®é™…æ›´æ–°è§„åˆ™: {updated} æ¡")

    if skipped:
        print(f"\nâš  éœ€ç¡®è®¤çš„è§„åˆ™ï¼š{len(skipped)} æ¡")
        # æŒ‰åŸå› åˆ†ç»„ç»Ÿè®¡
        reason_count = {}
        for item in skipped:
            reason = item.reason
            if reason not in reason_count:
                reason_count[reason] = []
            reason_count[reason].append(item)
        
        for reason, items in reason_count.items():
            print(f"  - {reason}: {len(items)} æ¡")
            for item in items[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªç¤ºä¾‹
                print(f"    ID {item.rule_id} ({item.rule_type})")
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        save_pending_rules(skipped, args.pending_json)

    if skipped_existing:
        print(f"\nâš  å·²å­˜åœ¨çš„è§„åˆ™ï¼š{len(skipped_existing)} æ¡")

    if args.dry_run and parsed:
        print("\nç¤ºä¾‹è§„åˆ™é¢„è§ˆï¼ˆå‰5æ¡ï¼‰ï¼š")
        for item in parsed[:5]:
            print(json.dumps(item.rule, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()

