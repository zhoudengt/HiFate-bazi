#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å†—ä½™ä»£ç æ£€æµ‹è„šæœ¬
æ£€æµ‹ç³»ç»Ÿä¸­çš„é‡å¤ä»£ç ã€æœªä½¿ç”¨ä»£ç ã€åŠŸèƒ½é‡å¤çš„æ¨¡å—
"""

import os
import re
import ast
import json
from pathlib import Path
from typing import Dict, List, Set, Tuple, Any
from collections import defaultdict
import hashlib

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent.parent


class RedundantCodeDetector:
    """å†—ä½™ä»£ç æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.issues = []
        self.duplicate_functions = defaultdict(list)
        self.duplicate_classes = defaultdict(list)
        self.duplicate_code_blocks = defaultdict(list)
        self.unused_imports = []
        self.similar_files = defaultdict(list)
        
    def detect_all(self):
        """æ‰§è¡Œæ‰€æœ‰æ£€æµ‹"""
        print("ğŸ” å¼€å§‹æ£€æµ‹å†—ä½™ä»£ç ...")
        
        # 1. æ£€æµ‹é‡å¤çš„å®¢æˆ·ç«¯å®ç°
        self._detect_duplicate_clients()
        
        # 2. æ£€æµ‹é‡å¤çš„ gRPC é…ç½®
        self._detect_duplicate_grpc_configs()
        
        # 3. æ£€æµ‹é‡å¤çš„å‡½æ•°å®šä¹‰
        self._detect_duplicate_functions()
        
        # 4. æ£€æµ‹é‡å¤çš„ä»£ç å—
        self._detect_duplicate_code_blocks()
        
        # 5. æ£€æµ‹ç›¸ä¼¼çš„æ–‡ä»¶
        self._detect_similar_files()
        
        # 6. æ£€æµ‹é‡å¤çš„æ ¼å¼åŒ–å‡½æ•°
        self._detect_format_functions()
        
        print(f"\nâœ… æ£€æµ‹å®Œæˆï¼Œå‘ç° {len(self.issues)} ä¸ªé—®é¢˜")
        
    def _detect_duplicate_clients(self):
        """æ£€æµ‹é‡å¤çš„å®¢æˆ·ç«¯å®ç°"""
        print("\nğŸ“¦ æ£€æµ‹é‡å¤çš„å®¢æˆ·ç«¯å®ç°...")
        
        # HTTP å’Œ gRPC å®¢æˆ·ç«¯å¯¹
        client_pairs = [
            ("src/clients/bazi_core_client.py", "src/clients/bazi_core_client_grpc.py", "BaziCoreClient"),
            ("src/clients/bazi_fortune_client.py", "src/clients/bazi_fortune_client_grpc.py", "BaziFortuneClient"),
            ("src/clients/bazi_rule_client.py", "src/clients/bazi_rule_client_grpc.py", "BaziRuleClient"),
        ]
        
        for http_file, grpc_file, class_name in client_pairs:
            http_path = PROJECT_ROOT / http_file
            grpc_path = PROJECT_ROOT / grpc_file
            
            if http_path.exists() and grpc_path.exists():
                self.issues.append({
                    "type": "duplicate_client",
                    "severity": "medium",
                    "description": f"å‘ç°é‡å¤çš„å®¢æˆ·ç«¯å®ç°ï¼š{class_name}",
                    "files": [http_file, grpc_file],
                    "suggestion": f"è€ƒè™‘ç»Ÿä¸€ä½¿ç”¨ gRPC å®¢æˆ·ç«¯ï¼ˆ{grpc_file}ï¼‰ï¼Œç§»é™¤ HTTP å®¢æˆ·ç«¯ï¼ˆ{http_file}ï¼‰"
                })
                print(f"  âš ï¸  {class_name}: HTTP å’Œ gRPC å®¢æˆ·ç«¯åŒæ—¶å­˜åœ¨")
    
    def _detect_duplicate_grpc_configs(self):
        """æ£€æµ‹é‡å¤çš„ gRPC é…ç½®"""
        print("\nâš™ï¸  æ£€æµ‹é‡å¤çš„ gRPC é…ç½®...")
        
        # æ ‡å‡†çš„ keepalive é…ç½®
        standard_keepalive = [
            ('grpc.keepalive_time_ms', 300000),
            ('grpc.keepalive_timeout_ms', 20000),
            ('grpc.keepalive_permit_without_calls', False),
        ]
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«è¿™äº›é…ç½®çš„æ–‡ä»¶
        config_pattern = r"grpc\.keepalive_time_ms.*?300000"
        files_with_config = []
        
        for py_file in PROJECT_ROOT.rglob("*.py"):
            try:
                content = py_file.read_text(encoding='utf-8')
                if re.search(config_pattern, content, re.DOTALL):
                    files_with_config.append(str(py_file.relative_to(PROJECT_ROOT)))
            except Exception:
                pass
        
        if len(files_with_config) > 1:
            self.issues.append({
                "type": "duplicate_grpc_config",
                "severity": "low",
                "description": f"å‘ç° {len(files_with_config)} ä¸ªæ–‡ä»¶åŒ…å«ç›¸åŒçš„ gRPC keepalive é…ç½®",
                "files": files_with_config[:10],  # åªæ˜¾ç¤ºå‰10ä¸ª
                "suggestion": "è€ƒè™‘å°† gRPC é…ç½®æå–åˆ°å…¬å…±å·¥å…·ç±»ä¸­ï¼Œç»Ÿä¸€ç®¡ç†"
            })
            print(f"  âš ï¸  å‘ç° {len(files_with_config)} ä¸ªæ–‡ä»¶åŒ…å«é‡å¤çš„ gRPC keepalive é…ç½®")
    
    def _detect_duplicate_functions(self):
        """æ£€æµ‹é‡å¤çš„å‡½æ•°å®šä¹‰"""
        print("\nğŸ”§ æ£€æµ‹é‡å¤çš„å‡½æ•°å®šä¹‰...")
        
        # æŸ¥æ‰¾æ‰€æœ‰ Python æ–‡ä»¶
        function_signatures = defaultdict(list)
        
        for py_file in PROJECT_ROOT.rglob("*.py"):
            # è·³è¿‡æµ‹è¯•æ–‡ä»¶å’Œç¼“å­˜æ–‡ä»¶
            if "test" in str(py_file) or "__pycache__" in str(py_file):
                continue
                
            try:
                content = py_file.read_text(encoding='utf-8')
                tree = ast.parse(content, filename=str(py_file))
                
                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        # ç”Ÿæˆå‡½æ•°ç­¾åï¼ˆåç§° + å‚æ•°æ•°é‡ï¼‰
                        sig = f"{node.name}({len(node.args.args)} args)"
                        function_signatures[sig].append(str(py_file.relative_to(PROJECT_ROOT)))
            except Exception:
                pass
        
        # æ‰¾å‡ºé‡å¤çš„å‡½æ•°ç­¾å
        for sig, files in function_signatures.items():
            if len(files) > 1:
                # æ£€æŸ¥æ˜¯å¦æ˜¯åŒä¸€ä¸ªæ–‡ä»¶ï¼ˆå¯èƒ½æ˜¯ç±»æ–¹æ³•ï¼‰
                unique_files = set(files)
                if len(unique_files) > 1:
                    self.duplicate_functions[sig].extend(unique_files)
        
        # æŠ¥å‘Šå‰10ä¸ªé‡å¤å‡½æ•°
        count = 0
        for sig, files in list(self.duplicate_functions.items())[:10]:
            if count >= 5:  # åªæŠ¥å‘Šå‰5ä¸ª
                break
            print(f"  âš ï¸  {sig}: åœ¨ {len(files)} ä¸ªæ–‡ä»¶ä¸­å‘ç°")
            count += 1
    
    def _detect_duplicate_code_blocks(self):
        """æ£€æµ‹é‡å¤çš„ä»£ç å—"""
        print("\nğŸ“‹ æ£€æµ‹é‡å¤çš„ä»£ç å—...")
        
        # æ£€æµ‹é‡å¤çš„åœ°å€è§£æé€»è¾‘ï¼ˆåœ¨ gRPC å®¢æˆ·ç«¯ä¸­ï¼‰
        address_parsing_pattern = r"if base_url\.startswith\([\"']http://[\"']\):.*?base_url = f\"\{base_url\}:(\d+)\""
        
        files_with_address_parsing = []
        for py_file in PROJECT_ROOT.rglob("*client*.py"):
            try:
                content = py_file.read_text(encoding='utf-8')
                if re.search(address_parsing_pattern, content, re.DOTALL):
                    files_with_address_parsing.append(str(py_file.relative_to(PROJECT_ROOT)))
            except Exception:
                pass
        
        if len(files_with_address_parsing) > 1:
            self.issues.append({
                "type": "duplicate_code_block",
                "severity": "medium",
                "description": f"å‘ç° {len(files_with_address_parsing)} ä¸ªæ–‡ä»¶åŒ…å«ç›¸åŒçš„åœ°å€è§£æé€»è¾‘",
                "files": files_with_address_parsing,
                "suggestion": "è€ƒè™‘å°†åœ°å€è§£æé€»è¾‘æå–åˆ°å…¬å…±å·¥å…·å‡½æ•°ä¸­"
            })
            print(f"  âš ï¸  å‘ç° {len(files_with_address_parsing)} ä¸ªæ–‡ä»¶åŒ…å«é‡å¤çš„åœ°å€è§£æé€»è¾‘")
    
    def _detect_similar_files(self):
        """æ£€æµ‹ç›¸ä¼¼çš„æ–‡ä»¶"""
        print("\nğŸ“„ æ£€æµ‹ç›¸ä¼¼çš„æ–‡ä»¶...")
        
        # æ£€æµ‹å®¢æˆ·ç«¯æ–‡ä»¶
        client_files = list(PROJECT_ROOT.glob("src/clients/*client*.py"))
        
        if len(client_files) >= 2:
            # è®¡ç®—æ–‡ä»¶ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„è¡Œæ•°æ¯”è¾ƒï¼‰
            file_sizes = {}
            for f in client_files:
                try:
                    lines = len(f.read_text(encoding='utf-8').splitlines())
                    file_sizes[str(f.relative_to(PROJECT_ROOT))] = lines
                except Exception:
                    pass
            
            # æ‰¾å‡ºå¤§å°ç›¸ä¼¼çš„æ–‡ä»¶
            similar_pairs = []
            files_list = list(file_sizes.items())
            for i, (file1, size1) in enumerate(files_list):
                for file2, size2 in files_list[i+1:]:
                    if abs(size1 - size2) < 50:  # è¡Œæ•°å·®å¼‚å°äº50
                        similar_pairs.append((file1, file2, size1, size2))
            
            if similar_pairs:
                self.issues.append({
                    "type": "similar_files",
                    "severity": "low",
                    "description": f"å‘ç° {len(similar_pairs)} å¯¹ç›¸ä¼¼çš„æ–‡ä»¶",
                    "files": [f"{f1} ({s1}è¡Œ) vs {f2} ({s2}è¡Œ)" for f1, f2, s1, s2 in similar_pairs[:5]],
                    "suggestion": "æ£€æŸ¥è¿™äº›æ–‡ä»¶æ˜¯å¦å¯ä»¥åˆå¹¶æˆ–æå–å…¬å…±ä»£ç "
                })
                print(f"  âš ï¸  å‘ç° {len(similar_pairs)} å¯¹ç›¸ä¼¼çš„æ–‡ä»¶")
    
    def _detect_format_functions(self):
        """æ£€æµ‹é‡å¤çš„æ ¼å¼åŒ–å‡½æ•°"""
        print("\nğŸ¨ æ£€æµ‹é‡å¤çš„æ ¼å¼åŒ–å‡½æ•°...")
        
        format_functions = []
        for py_file in PROJECT_ROOT.rglob("*.py"):
            try:
                content = py_file.read_text(encoding='utf-8')
                # æŸ¥æ‰¾æ ¼å¼åŒ–ç›¸å…³çš„å‡½æ•°
                if re.search(r"def.*format.*result|def.*format.*response|def.*_format", content, re.IGNORECASE):
                    tree = ast.parse(content, filename=str(py_file))
                    for node in ast.walk(tree):
                        if isinstance(node, ast.FunctionDef) and "format" in node.name.lower():
                            format_functions.append({
                                "file": str(py_file.relative_to(PROJECT_ROOT)),
                                "function": node.name,
                                "line": node.lineno
                            })
            except Exception:
                pass
        
        if len(format_functions) > 5:
            self.issues.append({
                "type": "duplicate_format_functions",
                "severity": "low",
                "description": f"å‘ç° {len(format_functions)} ä¸ªæ ¼å¼åŒ–å‡½æ•°",
                "files": [f"{f['file']}:{f['function']}" for f in format_functions[:10]],
                "suggestion": "è€ƒè™‘ç»Ÿä¸€æ ¼å¼åŒ–å‡½æ•°çš„å®ç°ï¼Œæå–åˆ°å…¬å…±å·¥å…·ç±»ä¸­"
            })
            print(f"  âš ï¸  å‘ç° {len(format_functions)} ä¸ªæ ¼å¼åŒ–å‡½æ•°ï¼Œå¯èƒ½å­˜åœ¨é‡å¤")
    
    def generate_report(self) -> str:
        """ç”Ÿæˆæ£€æµ‹æŠ¥å‘Š"""
        report_lines = [
            "# å†—ä½™ä»£ç æ£€æµ‹æŠ¥å‘Š\n",
            f"**æ£€æµ‹æ—¶é—´**: {self._get_timestamp()}\n",
            f"**å‘ç°é—®é¢˜æ•°**: {len(self.issues)}\n\n",
            "## ğŸ“Š é—®é¢˜æ±‡æ€»\n\n"
        ]
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»
        by_severity = defaultdict(list)
        for issue in self.issues:
            by_severity[issue["severity"]].append(issue)
        
        # é«˜ä¸¥é‡ç¨‹åº¦
        if "high" in by_severity:
            report_lines.append("### ğŸ”´ é«˜ä¸¥é‡ç¨‹åº¦\n\n")
            for issue in by_severity["high"]:
                report_lines.append(self._format_issue(issue))
        
        # ä¸­ç­‰ä¸¥é‡ç¨‹åº¦
        if "medium" in by_severity:
            report_lines.append("### ğŸŸ¡ ä¸­ç­‰ä¸¥é‡ç¨‹åº¦\n\n")
            for issue in by_severity["medium"]:
                report_lines.append(self._format_issue(issue))
        
        # ä½ä¸¥é‡ç¨‹åº¦
        if "low" in by_severity:
            report_lines.append("### ğŸŸ¢ ä½ä¸¥é‡ç¨‹åº¦\n\n")
            for issue in by_severity["low"]:
                report_lines.append(self._format_issue(issue))
        
        # å»ºè®®æ€»ç»“
        report_lines.append("\n## ğŸ’¡ ä¼˜åŒ–å»ºè®®æ€»ç»“\n\n")
        suggestions = set()
        for issue in self.issues:
            if "suggestion" in issue:
                suggestions.add(issue["suggestion"])
        
        for i, suggestion in enumerate(suggestions, 1):
            report_lines.append(f"{i}. {suggestion}\n")
        
        return "".join(report_lines)
    
    def _format_issue(self, issue: Dict) -> str:
        """æ ¼å¼åŒ–å•ä¸ªé—®é¢˜"""
        lines = [
            f"### {issue['description']}\n\n",
            f"- **ç±»å‹**: {issue['type']}\n",
            f"- **ä¸¥é‡ç¨‹åº¦**: {issue['severity']}\n",
            f"- **æ¶‰åŠæ–‡ä»¶**: {len(issue.get('files', []))} ä¸ª\n",
        ]
        
        if issue.get('files'):
            lines.append("\n**æ–‡ä»¶åˆ—è¡¨**:\n")
            for file in issue['files'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                lines.append(f"- `{file}`\n")
            if len(issue['files']) > 5:
                lines.append(f"- ... è¿˜æœ‰ {len(issue['files']) - 5} ä¸ªæ–‡ä»¶\n")
        
        if issue.get('suggestion'):
            lines.append(f"\n**å»ºè®®**: {issue['suggestion']}\n")
        
        lines.append("\n")
        return "".join(lines)
    
    def _get_timestamp(self) -> str:
        """è·å–æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def save_report(self, output_file: str = "redundant_code_report.md"):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        report = self.generate_report()
        output_path = PROJECT_ROOT / "docs" / "reports" / output_file
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        output_path.write_text(report, encoding='utf-8')
        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        
        # åŒæ—¶ä¿å­˜ JSON æ ¼å¼
        json_path = output_path.with_suffix('.json')
        json_path.write_text(
            json.dumps(self.issues, indent=2, ensure_ascii=False),
            encoding='utf-8'
        )
        print(f"ğŸ“„ JSON æŠ¥å‘Šå·²ä¿å­˜åˆ°: {json_path}")


def main():
    """ä¸»å‡½æ•°"""
    detector = RedundantCodeDetector()
    detector.detect_all()
    detector.save_report()
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š æ£€æµ‹æ‘˜è¦")
    print("="*60)
    print(f"æ€»é—®é¢˜æ•°: {len(detector.issues)}")
    
    by_type = defaultdict(int)
    for issue in detector.issues:
        by_type[issue['type']] += 1
    
    print("\næŒ‰ç±»å‹åˆ†ç±»:")
    for issue_type, count in sorted(by_type.items(), key=lambda x: -x[1]):
        print(f"  - {issue_type}: {count}")


if __name__ == "__main__":
    main()
