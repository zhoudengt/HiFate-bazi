# -*- coding: utf-8 -*-
"""
æœ¬åœ°æ„å›¾åˆ†ç±»å™¨ V2 - ä½¿ç”¨ sentence-transformers + ç›¸ä¼¼åº¦åŒ¹é…
å‡†ç¡®ç‡ç›®æ ‡ï¼š85%+
å“åº”æ—¶é—´ï¼š50-100ms
"""
import time
from typing import Dict, Any, List
import numpy as np

from services.intent_service.logger import logger
from services.intent_service.config import INTENT_CATEGORIES, INTENT_TO_RULE_TYPE_MAP

# å°è¯•å¯¼å…¥ sentence-transformers
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    logger.warning("sentence-transformers æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å…³é”®è¯å›é€€æ–¹æ¡ˆ")

# æ„å›¾æ¨¡æ¿ï¼ˆæ¯ä¸ªæ„å›¾çš„ä»£è¡¨æ€§é—®é¢˜ï¼‰
INTENT_TEMPLATES = {
    "wealth": [
        "æˆ‘çš„è´¢è¿æ€ä¹ˆæ ·ï¼Ÿ",
        "ä»Šå¹´é€‚åˆæŠ•èµ„å—ï¼Ÿ",
        "æˆ‘èƒ½èµšé’±å—ï¼Ÿ",
        "æˆ‘çš„è´¢å¯Œè¿åŠ¿å¦‚ä½•ï¼Ÿ",
        "æŠ•èµ„ç†è´¢æ€ä¹ˆæ ·ï¼Ÿ",
        "åè´¢æ­£è´¢å¦‚ä½•ï¼Ÿ",
        "ä»€ä¹ˆæ—¶å€™èƒ½å‘è´¢ï¼Ÿ",
        "è´¢è¿å¥½ä¸å¥½ï¼Ÿ"
    ],
    "career": [
        "æˆ‘çš„äº‹ä¸šè¿åŠ¿å¦‚ä½•ï¼Ÿ",
        "å·¥ä½œé¡ºåˆ©å—ï¼Ÿ",
        "èƒ½å‡èŒå—ï¼Ÿ",
        "äº‹ä¸šå‘å±•æ€ä¹ˆæ ·ï¼Ÿ",
        "é€‚åˆåˆ›ä¸šå—ï¼Ÿ",
        "èŒåœºè¿åŠ¿å¦‚ä½•ï¼Ÿ",
        "å·¥ä½œä¼šä¸ä¼šé¡ºåˆ©ï¼Ÿ",
        "äº‹ä¸šå¥½ä¸å¥½ï¼Ÿ"
    ],
    "marriage": [
        "æˆ‘çš„å©šå§»è¿åŠ¿å¦‚ä½•ï¼Ÿ",
        "ä»€ä¹ˆæ—¶å€™èƒ½ç»“å©šï¼Ÿ",
        "æ„Ÿæƒ…è¿åŠ¿æ€ä¹ˆæ ·ï¼Ÿ",
        "æ¡ƒèŠ±è¿å¦‚ä½•ï¼Ÿ",
        "å§»ç¼˜æ€ä¹ˆæ ·ï¼Ÿ",
        "æ‹çˆ±é¡ºåˆ©å—ï¼Ÿ",
        "é…å¶æƒ…å†µå¦‚ä½•ï¼Ÿ",
        "å©šå§»å¥½ä¸å¥½ï¼Ÿ"
    ],
    "health": [
        "æˆ‘çš„å¥åº·è¿åŠ¿å¦‚ä½•ï¼Ÿ",
        "èº«ä½“æ€ä¹ˆæ ·ï¼Ÿ",
        "ä¼šä¸ä¼šç”Ÿç—…ï¼Ÿ",
        "å¥åº·å¥½ä¸å¥½ï¼Ÿ",
        "ç–¾ç—…æƒ…å†µå¦‚ä½•ï¼Ÿ",
        "èº«ä½“ä¼šä¸ä¼šæœ‰é—®é¢˜ï¼Ÿ",
        "å…»ç”Ÿæ€ä¹ˆæ ·ï¼Ÿ",
        "å¥åº·è¿åŠ¿å¦‚ä½•ï¼Ÿ"
    ],
    "personality": [
        "æˆ‘çš„æ€§æ ¼ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æ€§æ ¼æ€ä¹ˆæ ·ï¼Ÿ",
        "è„¾æ°”å¦‚ä½•ï¼Ÿ",
        "å“æ€§å¦‚ä½•ï¼Ÿ",
        "æ€§æ ¼ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "ä¸ªæ€§æ€ä¹ˆæ ·ï¼Ÿ",
        "æ€§æ ¼å¥½ä¸å¥½ï¼Ÿ",
        "æ€§æ ¼ç‰¹å¾å¦‚ä½•ï¼Ÿ"
    ],
    "wangshui": [
        "æˆ‘çš„å‘½å±€æ—ºè¡°å¦‚ä½•ï¼Ÿ",
        "èº«æ—ºè¿˜æ˜¯èº«å¼±ï¼Ÿ",
        "äº”è¡Œå¼ºå¼±å¦‚ä½•ï¼Ÿ",
        "å‘½å±€æ—ºè¡°æ€ä¹ˆæ ·ï¼Ÿ",
        "æ—ºå¼±æƒ…å†µå¦‚ä½•ï¼Ÿ",
        "äº”è¡Œå¹³è¡¡å—ï¼Ÿ",
        "æ—ºè¡°å¦‚ä½•ï¼Ÿ",
        "å¼ºå¼±æ€ä¹ˆæ ·ï¼Ÿ"
    ],
    "yongji": [
        "æˆ‘çš„å–œç”¨ç¥æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¿Œç¥æ˜¯ä»€ä¹ˆï¼Ÿ",
        "ç”¨ç¥å¦‚ä½•ï¼Ÿ",
        "å–œå¿Œç¥æ€ä¹ˆæ ·ï¼Ÿ",
        "è°ƒå€™å¦‚ä½•ï¼Ÿ",
        "ç”¨ç¥å¥½ä¸å¥½ï¼Ÿ",
        "å–œç”¨ç¥æƒ…å†µå¦‚ä½•ï¼Ÿ",
        "å¿Œç¥æƒ…å†µå¦‚ä½•ï¼Ÿ"
    ],
    "shishen": [
        "æˆ‘çš„åç¥åˆ†æå¦‚ä½•ï¼Ÿ",
        "åç¥æƒ…å†µæ€ä¹ˆæ ·ï¼Ÿ",
        "æ­£å®˜åå®˜å¦‚ä½•ï¼Ÿ",
        "æ­£è´¢åè´¢å¦‚ä½•ï¼Ÿ",
        "é£Ÿç¥ä¼¤å®˜å¦‚ä½•ï¼Ÿ",
        "åç¥åˆ†ææ€ä¹ˆæ ·ï¼Ÿ",
        "åç¥å¥½ä¸å¥½ï¼Ÿ",
        "åç¥æƒ…å†µå¦‚ä½•ï¼Ÿ"
    ],
    "nayin": [
        "æˆ‘çš„çº³éŸ³æ˜¯ä»€ä¹ˆï¼Ÿ",
        "çº³éŸ³äº”è¡Œå¦‚ä½•ï¼Ÿ",
        "çº³éŸ³åˆ†ææ€ä¹ˆæ ·ï¼Ÿ",
        "çº³éŸ³æƒ…å†µå¦‚ä½•ï¼Ÿ",
        "çº³éŸ³å¥½ä¸å¥½ï¼Ÿ",
        "çº³éŸ³äº”è¡Œæ€ä¹ˆæ ·ï¼Ÿ",
        "çº³éŸ³åˆ†æå¦‚ä½•ï¼Ÿ",
        "çº³éŸ³æƒ…å†µå¦‚ä½•ï¼Ÿ"
    ],
    "general": [
        "æˆ‘çš„è¿åŠ¿æ€ä¹ˆæ ·ï¼Ÿ",
        "æ•´ä½“è¿åŠ¿å¦‚ä½•ï¼Ÿ",
        "å‘½ç†åˆ†æå¦‚ä½•ï¼Ÿ",
        "å…«å­—æ€ä¹ˆæ ·ï¼Ÿ",
        "å‘½ç›˜å¦‚ä½•ï¼Ÿ",
        "ç»¼åˆåˆ†ææ€ä¹ˆæ ·ï¼Ÿ",
        "æ•´ä½“æƒ…å†µå¦‚ä½•ï¼Ÿ",
        "è¿åŠ¿å¥½ä¸å¥½ï¼Ÿ"
    ]
}


class LocalIntentClassifierV2:
    """æœ¬åœ°æ„å›¾åˆ†ç±»å™¨ V2ï¼ˆä½¿ç”¨ sentence-transformers + ç›¸ä¼¼åº¦åŒ¹é…ï¼‰"""
    
    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        """
        åˆå§‹åŒ–æœ¬åœ°åˆ†ç±»å™¨
        
        Args:
            model_name: sentence-transformers æ¨¡å‹åç§°
                       æ¨èï¼šparaphrase-multilingual-MiniLM-L12-v2ï¼ˆé€Ÿåº¦å¿«ï¼Œå‡†ç¡®ç‡é«˜ï¼‰
        """
        self.model_name = model_name
        self.model = None
        self.intent_labels = list(INTENT_CATEGORIES.keys())
        self.rule_type_map = INTENT_TO_RULE_TYPE_MAP
        self.model_loaded = False
        self.intent_embeddings = {}  # ç¼“å­˜æ„å›¾æ¨¡æ¿çš„åµŒå…¥å‘é‡
        
        if SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                self._load_model()
                if self.model_loaded:
                    logger.info(f"[LocalIntentClassifierV2] âœ… åˆå§‹åŒ–æˆåŠŸï¼Œæ¨¡å‹: {model_name}")
                else:
                    logger.warning(f"[LocalIntentClassifierV2] âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨å…³é”®è¯å›é€€æ–¹æ¡ˆ")
            except Exception as e:
                logger.error(f"[LocalIntentClassifierV2] âŒ åˆå§‹åŒ–å¼‚å¸¸: {e}", exc_info=True)
                self.model_loaded = False
        else:
            logger.warning(f"[LocalIntentClassifierV2] âš ï¸ sentence-transformersåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å…³é”®è¯å›é€€æ–¹æ¡ˆ")
    
    def _load_model(self):
        """åŠ è½½ sentence-transformers æ¨¡å‹"""
        try:
            logger.info(f"[LocalIntentClassifierV2] å¼€å§‹åŠ è½½æ¨¡å‹: {self.model_name}")
            self.model = SentenceTransformer(self.model_name)
            self.model_loaded = True
            logger.info(f"[LocalIntentClassifierV2] âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # é¢„è®¡ç®—æ‰€æœ‰æ„å›¾æ¨¡æ¿çš„åµŒå…¥å‘é‡ï¼ˆæå‡æ€§èƒ½ï¼‰
            logger.info(f"[LocalIntentClassifierV2] é¢„è®¡ç®—æ„å›¾æ¨¡æ¿åµŒå…¥å‘é‡...")
            self._precompute_intent_embeddings()
            logger.info(f"[LocalIntentClassifierV2] âœ… æ„å›¾æ¨¡æ¿åµŒå…¥å‘é‡è®¡ç®—å®Œæˆ")
            
        except Exception as e:
            logger.error(f"[LocalIntentClassifierV2] âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
            self.model_loaded = False
    
    def _precompute_intent_embeddings(self):
        """é¢„è®¡ç®—æ‰€æœ‰æ„å›¾æ¨¡æ¿çš„åµŒå…¥å‘é‡"""
        for intent, templates in INTENT_TEMPLATES.items():
            # å°†æ‰€æœ‰æ¨¡æ¿åˆå¹¶ä¸ºä¸€ä¸ªæ–‡æœ¬ï¼ˆç”¨å¥å·åˆ†éš”ï¼‰
            combined_text = "ã€‚".join(templates)
            # è®¡ç®—åµŒå…¥å‘é‡
            embedding = self.model.encode(combined_text, convert_to_numpy=True)
            self.intent_embeddings[intent] = embedding
            logger.debug(f"[LocalIntentClassifierV2] é¢„è®¡ç®— {intent} çš„åµŒå…¥å‘é‡å®Œæˆ")
    
    def classify(
        self,
        question: str,
        use_keyword_fallback: bool = True
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ç›¸ä¼¼åº¦åŒ¹é…åˆ†ç±»æ„å›¾
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_keyword_fallback: å¦‚æœæ¨¡å‹ä¸å¯ç”¨ï¼Œæ˜¯å¦ä½¿ç”¨å…³é”®è¯å›é€€
        
        Returns:
            åˆ†ç±»ç»“æœ
        """
        request_id = f"local_v2_{int(time.time() * 1000)}"
        start_time = time.time()
        
        logger.info(f"[LocalIntentClassifierV2][{request_id}] ========== å¼€å§‹åˆ†ç±» ==========")
        logger.info(f"[LocalIntentClassifierV2][{request_id}] ğŸ“¥ è¾“å…¥: question={question}")
        
        # å¦‚æœæ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨å…³é”®è¯å›é€€
        if not SENTENCE_TRANSFORMERS_AVAILABLE or not self.model_loaded:
            logger.warning(f"[LocalIntentClassifierV2][{request_id}] âš ï¸ æ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨å…³é”®è¯å›é€€")
            if use_keyword_fallback:
                return self._keyword_based_classify(question, start_time, request_id)
            else:
                result = {
                    "intents": ["general"],
                    "confidence": 0.5,
                    "reasoning": "Model not available",
                    "is_ambiguous": True,
                    "response_time_ms": int((time.time() - start_time) * 1000),
                    "method": "fallback"
                }
                return result
        
        try:
            # ä½¿ç”¨ç›¸ä¼¼åº¦åŒ¹é…
            logger.info(f"[LocalIntentClassifierV2][{request_id}] [ç›¸ä¼¼åº¦åŒ¹é…] å¼€å§‹è®¡ç®—ç›¸ä¼¼åº¦...")
            similarity_start = time.time()
            
            # è®¡ç®—é—®é¢˜çš„åµŒå…¥å‘é‡
            question_embedding = self.model.encode(question, convert_to_numpy=True)
            
            # è®¡ç®—ä¸æ¯ä¸ªæ„å›¾çš„ç›¸ä¼¼åº¦
            similarities = {}
            for intent, intent_embedding in self.intent_embeddings.items():
                # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = np.dot(question_embedding, intent_embedding) / (
                    np.linalg.norm(question_embedding) * np.linalg.norm(intent_embedding)
                )
                similarities[intent] = float(similarity)
            
            similarity_time = int((time.time() - similarity_start) * 1000)
            logger.info(f"[LocalIntentClassifierV2][{request_id}] [ç›¸ä¼¼åº¦åŒ¹é…] ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ: è€—æ—¶={similarity_time}ms")
            
            # è·å–top-3æ„å›¾
            sorted_intents = sorted(similarities.items(), key=lambda x: x[1], reverse=True)
            top_intents = [intent for intent, score in sorted_intents[:3] if score > 0.3]  # é˜ˆå€¼è¿‡æ»¤
            
            if not top_intents:
                top_intents = [sorted_intents[0][0]]  # è‡³å°‘è¿”å›ä¸€ä¸ª
            
            # æœ€é«˜ç½®ä¿¡åº¦
            max_confidence = sorted_intents[0][1]
            
            # ğŸ”´ å¦‚æœç½®ä¿¡åº¦å¤ªä½ï¼ˆ<0.5ï¼‰ï¼Œä½¿ç”¨å…³é”®è¯å¢å¼º
            if max_confidence < 0.5:
                logger.warning(f"[LocalIntentClassifierV2][{request_id}] [ç›¸ä¼¼åº¦åŒ¹é…] âš ï¸ ç›¸ä¼¼åº¦è¾ƒä½ ({max_confidence:.2f})ï¼Œä½¿ç”¨å…³é”®è¯å¢å¼º")
                keyword_result = self._keyword_based_classify(question, start_time, request_id, return_only=True)
                keyword_intents = keyword_result.get("intents", [])
                keyword_confidence = keyword_result.get("confidence", 0.5)
                
                # åˆå¹¶ç»“æœï¼šä¼˜å…ˆä½¿ç”¨å…³é”®è¯çš„æ„å›¾ï¼Œä½†ä¿ç•™ç›¸ä¼¼åº¦ä¿¡æ¯
                if keyword_intents and keyword_confidence > 0.7:
                    top_intents = keyword_intents[:2] if len(keyword_intents) > 1 else keyword_intents
                    max_confidence = max(max_confidence, keyword_confidence * 0.9)  # ç¨å¾®é™ä½ï¼Œä¿ç•™ç›¸ä¼¼åº¦ç—•è¿¹
                    logger.info(f"[LocalIntentClassifierV2][{request_id}] [ç›¸ä¼¼åº¦åŒ¹é…] âœ… ä½¿ç”¨å…³é”®è¯å¢å¼º: intents={top_intents}, confidence={max_confidence:.2f}")
                else:
                    # å¦‚æœå…³é”®è¯ä¹Ÿä¸ç¡®å®šï¼Œæå‡ç›¸ä¼¼åº¦ç½®ä¿¡åº¦
                    max_confidence = max(max_confidence, 0.6)
                    logger.info(f"[LocalIntentClassifierV2][{request_id}] [ç›¸ä¼¼åº¦åŒ¹é…] âš ï¸ å…³é”®è¯ä¹Ÿä¸ç¡®å®šï¼Œæå‡ç½®ä¿¡åº¦è‡³{max_confidence:.2f}")
            else:
                logger.info(f"[LocalIntentClassifierV2][{request_id}] [ç›¸ä¼¼åº¦åŒ¹é…] âœ… ç›¸ä¼¼åº¦è¶³å¤Ÿé«˜: intents={top_intents}, confidence={max_confidence:.2f}")
            
            # æå–å…³é”®è¯
            keywords = self._extract_keywords(question)
            
            result = {
                "intents": top_intents[:2] if len(top_intents) > 1 else top_intents,
                "confidence": float(max_confidence),
                "keywords": keywords,
                "reasoning": f"Similarity-based classification: {', '.join(top_intents)}",
                "is_ambiguous": max_confidence < 0.75,
                "response_time_ms": int((time.time() - start_time) * 1000),
                "method": "sentence_transformer"
            }
            
            total_time = int((time.time() - start_time) * 1000)
            logger.info(f"[LocalIntentClassifierV2][{request_id}] ========== åˆ†ç±»å®Œæˆ ==========")
            logger.info(f"[LocalIntentClassifierV2][{request_id}] ğŸ“Š æ€»è€—æ—¶: {total_time}ms")
            logger.info(f"[LocalIntentClassifierV2][{request_id}] ğŸ“¤ æœ€ç»ˆè¾“å‡º: {result}")
            return result
            
        except Exception as e:
            logger.error(f"[LocalIntentClassifierV2][{request_id}] âŒ åˆ†ç±»å¤±è´¥: {e}", exc_info=True)
            if use_keyword_fallback:
                logger.warning(f"[LocalIntentClassifierV2][{request_id}] âš ï¸ é™çº§ä½¿ç”¨å…³é”®è¯å›é€€")
                return self._keyword_based_classify(question, start_time, request_id)
            else:
                result = {
                    "intents": ["general"],
                    "confidence": 0.5,
                    "reasoning": f"Classification error: {str(e)}",
                    "is_ambiguous": True,
                    "response_time_ms": int((time.time() - start_time) * 1000),
                    "method": "error"
                }
                return result
    
    def _keyword_based_classify(self, question: str, start_time: float, request_id: str = "", return_only: bool = False) -> Dict[str, Any]:
        """åŸºäºå…³é”®è¯çš„åˆ†ç±»ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        # æ„å›¾å…³é”®è¯æ˜ å°„ï¼ˆä»åŸä»£ç å¤åˆ¶ï¼‰
        intent_keywords = {
            "career": ["äº‹ä¸š", "å·¥ä½œ", "èŒä¸š", "å‡èŒ", "åˆ›ä¸š", "èŒåœº", "å‡è¿", "èŒä½"],
            "wealth": ["è´¢è¿", "è´¢å¯Œ", "èµšé’±", "æŠ•èµ„", "ç†è´¢", "å‘è´¢", "åè´¢", "æ­£è´¢", "æ”¶å…¥"],
            "marriage": ["å©šå§»", "æ„Ÿæƒ…", "æ‹çˆ±", "æ¡ƒèŠ±", "å§»ç¼˜", "é…å¶", "ç»“å©š", "åˆ†æ‰‹", "æ‹çˆ±"],
            "health": ["å¥åº·", "èº«ä½“", "ç–¾ç—…", "ç—…ç—‡", "å…»ç”Ÿ", "è„¾èƒƒ", "è‚èƒ†", "å¿ƒè„", "è‚¾", "è‚º"],
            "personality": ["æ€§æ ¼", "è„¾æ°”", "å“æ€§", "ç‰¹ç‚¹", "ä¼˜ç‚¹", "ç¼ºç‚¹", "ä¸ªæ€§"],
            "wangshui": ["æ—ºè¡°", "äº”è¡Œ", "å¼ºå¼±", "æ—ºå¼±", "èº«æ—º", "èº«å¼±"],
            "yongji": ["å–œç”¨ç¥", "å¿Œç¥", "ç”¨ç¥", "è°ƒå€™"],
            "shishen": ["åç¥", "æ­£å®˜", "ä¸ƒæ€", "æ­£è´¢", "åè´¢", "é£Ÿç¥", "ä¼¤å®˜", "æ­£å°", "åå°"],
            "nayin": ["çº³éŸ³", "çº³éŸ³äº”è¡Œ"],
            "general": ["è¿åŠ¿", "å‘½ç†", "å…«å­—", "å››æŸ±", "å‘½ç›˜", "æ€ä¹ˆæ ·", "å¦‚ä½•"]
        }
        
        matched_intents = []
        matched_keywords = []
        
        for intent, keywords in intent_keywords.items():
            for keyword in keywords:
                if keyword in question:
                    if intent not in matched_intents:
                        matched_intents.append(intent)
                    if keyword not in matched_keywords:
                        matched_keywords.append(keyword)
        
        if not matched_intents:
            matched_intents = ["general"]
            confidence = 0.75
        elif len(matched_intents) == 1:
            confidence = 0.95
        else:
            confidence = 0.90
        
        # æ—¶é—´å…³é”®è¯å¢å¼º
        time_keywords = ["ä»Šå¹´", "æ˜å¹´", "åå¹´", "ä»Šå¹´", "æœ¬æœˆ", "ä»Šå¤©", "ä»Šå¹´", "æ˜å¹´", "åä¸‰å¹´", "æœªæ¥", "å", "å¹´"]
        if any(kw in question for kw in time_keywords):
            confidence = min(confidence + 0.05, 0.98)
        
        # å¼ºæ„å›¾å…³é”®è¯å¢å¼º
        strong_intent_keywords = ["æŠ•èµ„", "ç†è´¢", "å‘è´¢", "èµšé’±", "å‡èŒ", "åˆ›ä¸š", "ç»“å©š", "æ‹çˆ±", "å¥åº·", "èº«ä½“", "è´¢è¿", "äº‹ä¸š", "å©šå§»"]
        if any(kw in question for kw in strong_intent_keywords):
            confidence = min(confidence + 0.03, 0.98)
        
        result = {
            "intents": matched_intents,
            "confidence": confidence,
            "keywords": matched_keywords[:5],
            "reasoning": f"Keyword-based classification: {', '.join(matched_intents)}",
            "is_ambiguous": len(matched_intents) > 2,
            "response_time_ms": int((time.time() - start_time) * 1000),
            "method": "keyword_fallback"
        }
        
        if not return_only:
            logger.info(f"[LocalIntentClassifierV2][{request_id}] [å…³é”®è¯å›é€€] âœ… å…³é”®è¯åˆ†ç±»å®Œæˆ: {result}")
        
        return result
    
    def _extract_keywords(self, question: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦å¢å¼ºï¼‰
        keywords = []
        for intent, templates in INTENT_TEMPLATES.items():
            for template in templates:
                # æå–æ¨¡æ¿ä¸­çš„å…³é”®è¯
                for word in ["è´¢è¿", "äº‹ä¸š", "å©šå§»", "å¥åº·", "æ€§æ ¼", "æŠ•èµ„", "å·¥ä½œ", "æ„Ÿæƒ…"]:
                    if word in template and word in question:
                        if word not in keywords:
                            keywords.append(word)
        return keywords[:5]

