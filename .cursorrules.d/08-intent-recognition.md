# 意图识别混合架构规范 【必须遵守】

## 🔴 核心原则

> **意图识别响应时间必须 < 1秒，使用混合架构实现高性能。**

## 📋 架构设计

**混合架构流程**：
```
用户输入
    ↓
【第1层：关键词过滤】（0ms，处理60%的明确问题）
    ├─ 强指示词 → 直接通过（99%准确）
    ├─ 黑名单 → 直接拒绝（95%准确）
    └─ 白名单 → 直接通过（90%准确）
    ↓
【第2层：本地BERT模型】（50-100ms，处理20%的简单问题）
    ├─ 模型分类（如果可用）
    └─ 关键词回退（如果模型不可用）
    ↓
【第3层：判断是否需要LLM兜底】
    ├─ 置信度 < 0.6 → LLM
    ├─ 问题模糊 → LLM
    └─ 复杂时间表达 → LLM
    ↓
【第4层：规则后处理】（10-20ms）
    ├─ 时间意图解析（支持中文数字）
    ├─ 多意图合并
    └─ JSON格式化
    ↓
【第5层：LLM兜底】（500-1000ms，仅处理5%的模糊问题）
    └─ 仅处理复杂/模糊问题
    ↓
最终结果
```

## 🎯 性能要求

| 场景 | 占比 | 响应时间 | 准确率 | 使用技术 |
|------|------|---------|--------|---------|
| 关键词明确 | 60% | <10ms | 95%+ | 关键词过滤 |
| 简单问题 | 20% | 50-100ms | 85-90% | 本地模型 + 规则 |
| 复杂问题 | 15% | 100-200ms | 80-85% | 本地模型 + 规则 |
| 模糊问题 | 5% | 500-1000ms | 90%+ | LLM兜底 |

**平均响应时间**：< 100ms（满足 < 1秒要求）

## 📁 核心文件

| 文件 | 作用 | 说明 |
|------|------|------|
| `services/intent_service/local_classifier.py` | 本地BERT模型分类器 | 处理简单问题（50-100ms） |
| `services/intent_service/rule_postprocessor.py` | 规则后处理器 | 时间意图解析、JSON格式化（10-20ms） |
| `services/intent_service/classifier.py` | 混合架构路由 | 智能路由到不同处理层 |
| `services/intent_service/question_filter.py` | 关键词过滤器 | 快速过滤明确问题（0ms） |
| `services/intent_service/llm_client.py` | LLM客户端 | 兜底处理复杂问题（500-1000ms） |

## 🔧 实现要求

### 1. 本地模型分类器

**要求**：
- 使用BERT/RoBERTa中文模型（可选，如果不可用则使用关键词回退）
- 支持关键词回退方案（确保模型不可用时仍能工作）
- 响应时间 < 100ms

### 2. 规则后处理器

**要求**：
- 支持中文数字和阿拉伯数字的时间表达
- 支持多种时间类型（今天、本月、今年、明年、后N年、XXXX年、XXXX-YYYY年等）
- 响应时间 < 20ms

**时间意图类型**：
- `today` - 今天/今日
- `this_month` - 本月/这个月
- `this_year` - 今年/本年（默认）
- `next_year` - 明年（只有1年）
- `future_years` - 后N年/未来N年（N年）
- `recent_years` - 最近N年
- `specific_year` - XXXX年（单个年份）
- `year_range` - XXXX-YYYY年（年份范围）

### 3. 混合架构路由

**要求**：
- 智能判断是否需要LLM兜底
- 优先使用本地模型，仅在必要时调用LLM
- 确保平均响应时间 < 100ms

**LLM兜底条件**：
- 本地模型置信度 < 0.6
- 问题过于模糊（长度 < 5 或缺少关键词）
- 多意图冲突（意图数量 > 2 且置信度低）
- 复杂时间表达（需要上下文理解）

## ⚙️ 配置项

在 `services/intent_service/config.py` 中：

```python
# 混合架构配置
HYBRID_ARCHITECTURE_ENABLED = os.getenv("HYBRID_ARCHITECTURE_ENABLED", "true").lower() == "true"
LOCAL_MODEL_NAME = os.getenv("LOCAL_MODEL_NAME", "hfl/chinese-roberta-wwm-ext")
LLM_FALLBACK_THRESHOLD = float(os.getenv("LLM_FALLBACK_THRESHOLD", "0.6"))  # 置信度阈值
```

## ✅ 检查清单

每次修改意图识别相关代码时，必须检查：

- [ ] 平均响应时间是否 < 100ms
- [ ] 是否优先使用本地模型/关键词过滤
- [ ] LLM兜底是否仅在必要时调用（< 5%的情况）
- [ ] 时间意图识别是否支持中文数字
- [ ] 是否支持所有时间类型（今天、明年、后N年、XXXX年等）
- [ ] 错误处理是否完善（模型不可用时使用回退方案）

## 🚨 常见问题

### 问题1：时间意图识别失败

**症状**："后三年"无法识别为`future_years`

**原因**：正则表达式不支持中文数字

**解决**：在`rule_postprocessor.py`中支持中文数字转换

### 问题2：响应时间过长

**症状**：平均响应时间 > 1秒

**原因**：过多调用LLM API

**解决**：
- 降低LLM兜底阈值（`LLM_FALLBACK_THRESHOLD`）
- 优化关键词过滤规则
- 增强本地模型能力

### 问题3：本地模型不可用

**症状**：`transformers`未安装或模型加载失败

**解决**：自动使用关键词回退方案，确保服务可用

**核心要点**：
- **意图识别必须使用混合架构，响应时间 < 1秒**
- **优先使用本地模型/关键词过滤，LLM仅作为兜底**
- **时间意图识别必须支持中文数字和阿拉伯数字**
- **所有时间类型必须正确识别（今天、明年、后N年、XXXX年等）**

