# Logstash pipeline: 解析 stream_flow 日志并写入 Elasticsearch
# 输入：Filebeat 发送的 JSON 日志
#
# 性能优化：
# - 使用 pipeline.batch.size 和 pipeline.batch.delay 控制批量写入
# - 条件判断避免无效字段赋值

input {
  beats {
    port => 5044
  }
}

filter {
  # 解析 JSON 消息
  json {
    source => "message"
    target => "log"
    skip_on_invalid_json => true
    tag_on_failure => ["_jsonparsefailure"]
  }

  # JSON 解析失败时跳过后续处理
  if "_jsonparsefailure" in [tags] {
    mutate {
      add_field => { "parse_error" => "true" }
    }
  } else {
    # 解析时间戳（仅当字段存在时）
    if [log][timestamp] {
      date {
        match => ["[log][timestamp]", "ISO8601"]
        target => "@timestamp"
      }
    }

    # 提升常用字段到顶层便于 Kibana 查询（仅当字段存在时）
    if [log][trace_id] {
      mutate {
        add_field => { "trace_id" => "%{[log][trace_id]}" }
      }
    }
    if [log][stage] {
      mutate {
        add_field => { "stage" => "%{[log][stage]}" }
      }
    }
    if [log][endpoint] {
      mutate {
        add_field => { "endpoint" => "%{[log][endpoint]}" }
      }
    }

    # 移除原始 message 节省存储（JSON 已解析到 log 字段）
    mutate {
      remove_field => ["message"]
    }
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "stream-flow-%{+YYYY.MM.dd}"
    # 启用批量写入优化性能
    manage_template => false
  }
}
