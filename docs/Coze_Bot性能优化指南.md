# Coze Bot 性能优化指南

> **目标**：在保证分析深度的前提下，将响应速度从 25秒 优化到 10-12秒

---

## 📊 性能优化概览

| 优化项 | 当前状态 | 优化目标 | 预期提升 | 是否影响深度 |
|--------|---------|---------|---------|------------|
| Prompt长度 | 8000字 | 2000字 | -10秒 | ❌ 不影响 |
| Temperature | 0.9 | 0.6-0.7 | -2秒 | ❌ 不影响 |
| Max Tokens | 4096 | 2048 | -3秒 | ⚠️ 需测试 |
| Top P | 1.0 | 0.85 | -1秒 | ❌ 不影响 |
| **总计** | **25秒** | **10-12秒** | **-50%** | ✅ 保持深度 |

---

## ⚙️ Bot 参数配置（Coze平台操作）

### 1. 基础配置

访问您的 Coze Bot：https://www.coze.cn/space/7565058187868176436/bot/7576211240901509174

### 2. 模型设置（Model Settings）

```yaml
模型选择: 
  - 推荐：Claude 3.5 Sonnet (速度快，深度好)
  - 备选：GPT-4o (稍慢但更稳定)

Temperature (创造性): 0.6
  解释：降低随机性，减少无关输出，加快生成
  范围：0.0-1.0
  当前：0.9 → 优化：0.6-0.7
  
Max Tokens (最大输出): 2048
  解释：限制输出长度，避免冗余
  范围：512-4096
  当前：4096 → 优化：2048
  ⚠️ 注意：太短会截断分析，需测试

Top P (核心采样): 0.85
  解释：只采样最可能的85%词汇，减少发散
  范围：0.0-1.0
  当前：1.0 → 优化：0.85

Frequency Penalty (重复惩罚): 0.3
  解释：减少重复表述，提高信息密度
  范围：0.0-2.0
  推荐：0.3

Presence Penalty (主题惩罚): 0.1
  解释：鼓励聚焦主题，避免跑题
  范围：0.0-2.0
  推荐：0.1
```

### 3. Prompt 优化（System Prompt）

**当前Prompt问题**：
- ❌ 过长（8000字）→ 处理慢
- ❌ 过多格式要求 → 机械化
- ❌ 示例过长（5000字）→ 浪费token

**优化后Prompt特点**：
- ✅ 精简（2000字）→ 处理快
- ✅ 保留核心要求 → 不失深度
- ✅ 短小示例 → 节省token

**如何应用**：
1. 打开您的Bot编辑页面
2. 找到 "System Prompt" 或"系统提示词"部分
3. 复制 `docs/Coze_Bot配置文档-命理分析专家-精简版.md` 中的Prompt
4. 粘贴替换原有Prompt
5. 点击"保存"并"发布"到 API 渠道

### 4. 响应格式（Response Format）

```yaml
输出格式: JSON (structured output)
  好处：解析快，错误少

流式输出: 开启 (如果Coze支持)
  好处：用户体验好，感知速度快
  注意：需要后端代码配合
```

---

## 🎯 优化步骤（按顺序执行）

### 步骤1：应用精简版Prompt（最重要）⭐

**操作**：
1. 访问 Coze Bot 编辑页面
2. 复制 `docs/Coze_Bot配置文档-命理分析专家-精简版.md` 中的Prompt
3. 替换到 Bot 的 System Prompt
4. 保存并发布

**预期效果**：
- 响应时间：25秒 → 12-15秒
- 分析深度：保持或提升（更自然）

**验证方式**：
```bash
cd /Users/zhoudt/Downloads/project/HiFate-bazi
./test_deep_analysis.sh
```

---

### 步骤2：调整模型参数

**操作**（在Coze平台）：

```
【模型设置】
Temperature: 0.9 → 0.6
Max Tokens: 4096 → 2048（⚠️ 先测试2500，再逐步降低）
Top P: 1.0 → 0.85
Frequency Penalty: 0 → 0.3
Presence Penalty: 0 → 0.1
```

**预期效果**：
- 响应时间：再减 2-3秒
- 分析深度：不受影响（参数调整不改变理解能力）

**⚠️ 重要**：Max Tokens 不要一步降到2048，建议：
1. 先降到 3000，测试是否截断
2. 再降到 2500，观察输出质量
3. 最后降到 2048

---

### 步骤3：测试验证（必须）

**测试命令**：
```bash
cd /Users/zhoudt/Downloads/project/HiFate-bazi

# 测试1：速度测试
time curl -X GET "http://localhost:8000/api/v1/smart-fortune/analyze?year=1990&month=5&day=15&hour=14&gender=male&question=我明年的财运如何？&include_fortune_context=true"

# 测试2：深度测试（检查关键词）
./test_deep_analysis.sh

# 测试3：对比测试（优化前后对比）
# 记录优化前的输出 → 保存到文件
# 优化后再测试 → 对比输出质量
```

**检查清单**：
- ✅ 响应时间 < 15秒
- ✅ 分析包含"因果关系"
- ✅ 分析包含"五行生克"
- ✅ 分析包含"十神关系"
- ✅ 有具体建议
- ✅ 输出完整（无截断）

---

## 📈 性能对比表（填写实际测试结果）

| 测试场景 | 优化前（秒） | 优化后（秒） | 提升 | 深度评分 |
|---------|------------|------------|-----|---------|
| 明年财运 | 25 | ___ | ___% | ___/10 |
| 后年事业 | 23 | ___ | ___% | ___/10 |
| 今年健康 | 27 | ___ | ___% | ___/10 |
| 婚姻运势 | 24 | ___ | ___% | ___/10 |
| **平均** | **24.75** | **___** | **___% | **___/10** |

**深度评分标准**（1-10分）：
- 10分：深度因果推理 + 具体场景 + 量化分析 + 个性化建议
- 8分：因果推理 + 具体场景 + 建议
- 6分：有解释 + 基本建议
- 4分：简单描述
- 2分：只有结论

---

## 🔧 故障排查

### 问题1：优化后响应时间没变

**可能原因**：
1. Prompt 没有保存/发布
2. 调用的是旧版本Bot
3. 缓存问题

**解决方案**：
```bash
# 1. 检查Bot版本
curl -X GET "http://localhost:8000/api/v1/smart-fortune/analyze?question=测试&year=1990&month=1&day=1&hour=12&gender=male" | jq '.bot_version'

# 2. 清理Redis缓存
redis-cli -p 16379 FLUSHDB

# 3. 重启服务
./restart_server.sh
```

---

### 问题2：优化后分析变浅了

**可能原因**：
1. Max Tokens 设置过低（截断）
2. Temperature 过低（太死板）
3. Prompt 丢失了关键要求

**解决方案**：
1. 检查输出是否被截断：
   ```bash
   # 查看输出长度
   curl ... | jq '.llm_deep_analysis' | wc -c
   # 如果接近 Max Tokens 限制，说明被截断
   ```

2. 调整参数：
   - Max Tokens: 2048 → 3000
   - Temperature: 0.6 → 0.7

3. 对比 Prompt：
   - 确保保留了"因果链三步法"
   - 确保保留了"五行生克"要求
   - 确保保留了"个性化建议"要求

---

### 问题3：响应速度不稳定

**可能原因**：
1. Coze API 服务波动
2. 网络延迟
3. Bot 负载高

**解决方案**：
1. 启用Redis缓存（阶段3会实现）
2. 添加重试机制
3. 监控API响应时间：
   ```python
   # 在 fortune_llm_client.py 中添加
   import time
   start = time.time()
   response = coze_api.call()
   duration = time.time() - start
   logger.info(f"Coze API响应时间: {duration:.2f}秒")
   ```

---

## 📝 优化记录（请填写）

### 优化1：精简Prompt

- 日期：2025-11-25
- 操作：应用精简版Prompt（2000字）
- 结果：
  - 响应时间：___ 秒 → ___ 秒
  - 深度评分：___ / 10
  - 问题：___（如有）

### 优化2：调整模型参数

- 日期：____
- 操作：
  - Temperature: ___ → ___
  - Max Tokens: ___ → ___
  - Top P: ___ → ___
- 结果：
  - 响应时间：___ 秒 → ___ 秒
  - 深度评分：___ / 10
  - 问题：___（如有）

### 优化3：其他

- 日期：____
- 操作：___
- 结果：___

---

## 🎓 最佳实践

### 原则1：先测试，后部署

- ❌ 不要一次改多个参数
- ✅ 每次只改1个参数，测试效果
- ✅ 记录每次改动的结果

### 原则2：深度优先于速度

- ❌ 不要为了速度牺牲分析质量
- ✅ 如果优化后变浅，立即回滚
- ✅ 目标是"又快又深"，不是"快但浅"

### 原则3：用户感知最重要

- ✅ 10秒 + 流式输出 > 8秒阻塞
- ✅ 12秒深度分析 > 5秒浅分析
- ✅ 添加进度提示（"正在分析八字..."）

---

## 📞 需要帮助？

如果优化后遇到问题：

1. 查看日志：
   ```bash
   tail -f logs/server.log | grep "LLM深度分析"
   ```

2. 运行诊断：
   ```bash
   cd /Users/zhoudt/Downloads/project/HiFate-bazi
   python3 scripts/diagnose_bot_performance.py
   ```

3. 检查 Bot 配置：
   - 确认已发布到 API 渠道
   - 确认 Prompt 正确
   - 确认参数设置正确

---

**记住**：优化的目标是"快速 + 深度"，而不是"快速 - 深度"！

